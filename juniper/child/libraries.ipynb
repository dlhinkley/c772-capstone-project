{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Temp for local development\n",
    "import ssl\n",
    "import os\n",
    "# os.environ['PYSPARK_PYTHON'] = '/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6'\n",
    "import tinydb as tinydb\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip3 install tinydb"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from pyspark import SparkContext, SparkFiles, SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "# https://github.com/shakedzy/dython\n",
    "from dython.nominal import associations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SQLContext(sc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "dataDir = '/Users/duane.hinkley/PycharmProjects/c772-capstone-project/juniper/.data/'\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.makedirs(dataDir)\n",
    "\n",
    "def save_df(df, name):\n",
    "    df.repartition(1).write.mode('overwrite').parquet(dataDir + name + \".parquet\")\n",
    "\n",
    "def save_dict(data, name):\n",
    "    with open(dataDir + name + \".json\", \"w\") as f:\n",
    "      json.dump(data, f)\n",
    "\n",
    "def load_dict(name):\n",
    "    with open(dataDir + name + \".json\") as f:\n",
    "        out = json.load(f)\n",
    "\n",
    "    return out\n",
    "\n",
    "def load_df(name):\n",
    "    return spark.read.parquet(dataDir + name + \".parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_non_string_vars():\n",
    "    global shared\n",
    "    return shared['identifierVars'] + shared['continousVars'] + shared['intervalVars'] + shared['binaryVars']\n",
    "\n",
    "def get_all_vars():\n",
    "        return shared['nominalVars'] + get_non_string_vars();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tinydb import TinyDB, Query\n",
    "# Create Todo list\n",
    "def init_todo():\n",
    "  global td, dataDir\n",
    "  td = TinyDB(dataDir + 'todo.json')\n",
    "\n",
    "def add_todo(desc):\n",
    "  global td\n",
    "  q = Query()\n",
    "  if td.contains(q.todo == desc):\n",
    "      td.insert({'todo': desc, 'finished': False})\n",
    "\n",
    "def list_todo():\n",
    "  global td\n",
    "  for item in td:\n",
    "     print(item)\n",
    "\n",
    "def finish_todo(desc):\n",
    "  global td\n",
    "  q = Query()\n",
    "  td.update({'finished': True}, q.todo == desc)\n",
    "\n",
    "\n",
    "init_todo()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkFiles\n",
    "from datetime import datetime\n",
    "\n",
    "def import_by_url(url):\n",
    "  # Given a url to a csv file, import and return a dataframe\n",
    "  #\n",
    "  sc.addFile(url)\n",
    "  filename = os.path.basename(url)\n",
    "  file = \"file://\" + SparkFiles.get(filename)\n",
    "  return spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(file)\n",
    "\n",
    "\n",
    "def filter_default(dfIn, f1, f2):\n",
    "  # Given a dataframe and two date field names, returns the dataframe removing records\n",
    "  # where the f1 or f2 columns equal a default date\n",
    "  defaultDates = [\"2999-01-01 00:00:00\", \"1900-01-01 00:00:00\"]\n",
    "  return dfIn.filter( ~F.col(f1).isin(defaultDates) & ~F.col(f2).isin(defaultDates) )\n",
    "\n",
    "\n",
    "def date_stats(dfIn, f1, f2):\n",
    "  # Given a dataframe and two date field names, returns a new dataframe with the difference between\n",
    "  # the dates in minutes, hours and minutes\n",
    "  dfOut = filter_default(dfIn, f1, f2)\n",
    "\n",
    "  dfOut = dfOut.withColumn(\"minues\", (F.col(f1).cast(\"long\") - F.col(f2).cast(\"long\"))/60.).select(f1, f2, \"minues\")\n",
    "\n",
    "  dfOut = dfOut.withColumn(\"hours\", (F.col(f1).cast(\"long\") - F.col(f2).cast(\"long\"))/3600.).select(f1, f2, \"hours\", \"minues\")\n",
    "\n",
    "  return dfOut.withColumn(\"days\", (F.col(f1).cast(\"long\") - F.col(f2).cast(\"long\"))/86400.).select(\"days\", \"hours\", \"minues\")\n",
    "\n",
    "\n",
    "def annotate_plot(ax):\n",
    "  # Add total labels to plot\n",
    "  for p in ax.patches:\n",
    "      ax.annotate(\n",
    "        round(p.get_height(), 2),\n",
    "        (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        color='white',\n",
    "        fontweight='bold',\n",
    "        xytext=(0, -10),\n",
    "        textcoords='offset points')\n",
    "\n",
    "\n",
    "def date_boxplot(pdDf, title):\n",
    "    # Given a dataframe of dates, create a boxplot of\n",
    "    # date distribution\n",
    "\n",
    "    ax = pdDf.boxplot(rot=270, figsize=[10,10])\n",
    "\n",
    "    max = pdDf.max().max()\n",
    "    min = pdDf.min().min()\n",
    "    ytick = np.linspace(start=min, stop=max, num=12)\n",
    "    newLabels = [datetime.fromtimestamp(ts).strftime('%Y-%m-%d') for ts in ytick]\n",
    "    ax.set_yticks(ytick)\n",
    "    ax.set_yticklabels(labels=newLabels)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def single_val(df):\n",
    "    \"\"\" Give a dataframe, and a column to group by return a list of\n",
    "        single value variables\n",
    "    \"\"\"\n",
    "\n",
    "    inCols = []\n",
    "\n",
    "    for c in df.columns:\n",
    "        if df[c].unique().size == 1:\n",
    "           inCols.append(c)\n",
    "\n",
    "    return inCols\n",
    "\n",
    "def display_sv_cols(df, cols):\n",
    "    return df.select(cols).toPandas().head(1).transpose()\n",
    "\n",
    "def id_to_name(df, idVar, newVar, newIdList):\n",
    "  # Given a dataframe, id variable, new variable name and list of new ids\n",
    "  # add a new variable to the dataframe mapping the id to the array\n",
    "\n",
    "  # Save org ids to a list\n",
    "  oldIdList = [row[idVar] for row in df.select(idVar).distinct().orderBy(idVar).collect()]\n",
    "\n",
    "    # Create map\n",
    "  newIdMap = dict()\n",
    "  # Add letters to map\n",
    "  for i, val in enumerate(oldIdList):\n",
    "      newIdMap[val] = newIdList[i]\n",
    "\n",
    "  # Create mapping expression\n",
    "  mapping_expr = F.create_map([F.lit(x) for x in chain(*newIdMap.items())])\n",
    "\n",
    "  # Add org column with letter related to id\n",
    "  return df.withColumn(newVar, mapping_expr[df[idVar]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}