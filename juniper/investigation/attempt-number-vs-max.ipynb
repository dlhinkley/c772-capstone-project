{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Investigate binary variables assignment_attempt_number and assignment_max_attempts\n",
    "Appears binary but the variable name indicates it could have any values. The data only contains 1 and 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def num_max_cross():\n",
    "    dfPd = dfInv.toPandas()\n",
    "    # Return cross-tabulation table of assignment_attempt_number vs assignment_max_attempts adding counts for null values\n",
    "    pd.crosstab(dfPd.assignment_attempt_number.fillna('null'), dfPd.assignment_max_attempts.fillna('null'), margins=True, margins_name=\"Total\")\n",
    "\n",
    "num_max_cross()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- both fields only contain \"0\" 1566 out of 78,982 observations, otherwise the value is always \"1\"\n",
    "- are they the same 1566 as the \"[unassigned]\" in response_correctness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Variables Perfectly Correlated to assignment_attempt_number = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# def zero_attempt_cor():\n",
    "# Only assignment_attempt_number == 0 observations\n",
    "dfPd = dfInv.filter( F.col('assignment_attempt_number') == 0 ).select(*get_all_vars()).toPandas()\n",
    "# Convert all non string variables to string\n",
    "for f in get_non_string_vars():\n",
    "    dfPd[f] = dfPd[f].astype(str)\n",
    "\n",
    "# Display heatmap\n",
    "assoc = associations( dfPd, figsize=[20,20],bias_correction=False )\n",
    "# return assoc['corr']\n",
    "\n",
    "\n",
    "corDf = assoc['corr']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print perfect correlations\n",
    "cols = corDf.columns.values\n",
    "pairsDf = pd.DataFrame(columns = ['name1', 'name2'])\n",
    "\n",
    "for (columnName, columnData) in corDf.iteritems():\n",
    "    #print ('colname', columnName)\n",
    "    for i, value in enumerate(columnData):\n",
    "      # If perfect correlation and not the same column\n",
    "      if cols[i] != columnName and value == 1:\n",
    "        #print ('rowname', cols[i])\n",
    "        #print ('value', value)\n",
    "        # If not in list\n",
    "        # if (pairsDf['name1'] == columnName) and (pairsDf['name2'] == cols[i]):\n",
    "        if pairsDf[((pairsDf['name1'] == columnName) & (pairsDf['name2'] == cols[i]))].empty and pairsDf[((pairsDf['name2'] == columnName) & (pairsDf['name1'] == cols[i]))].empty:\n",
    "          pairsDf = pairsDf.append({'name1' : columnName, 'name2' : cols[i]}, ignore_index=True)\n",
    "          print(pd.crosstab(dfPd[columnName], dfPd[cols[i]]))\n",
    "\n",
    "# pairsDf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- scoring_type_code indicates it was manually scored\n",
    "- what's the number of assessments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Get Counts on the 1566 here assignment_attempt_number = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfZeroAttempt.agg(\n",
    "      F.countDistinct('section_id').alias('sections'),\n",
    "      F.countDistinct('learner_id').alias('learners'),\n",
    "      F.countDistinct('assessment_id').alias('assessments'),\n",
    "      F.countDistinct('assessment_item_response_id').alias('items'),\n",
    "      F.sum( F.when( F.col('response_correctness')      == '[unassigned]', 1).otherwise(0)).alias('response_correctness'),\n",
    "      F.sum( F.when( F.col('assignment_attempt_number') == 0, 1).otherwise(0)).alias('assignment_attempt_number'),\n",
    "      F.sum( F.when( F.col('assignment_max_attempts')   == 0, 1).otherwise(0)).alias('assignment_max_attempts')\n",
    "   ).show(1, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "finish_todo(\"Investigate binary variables assignment_attempt_number and assignment_max_attempts\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}