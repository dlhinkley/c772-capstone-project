{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##### Investigate 3298 null values in response_correctness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Check correlations\n",
    "- Missing at random?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "associations( dfFlt.filter(F.col('response_correctness').isNull() ).select(*binaryVars, *nominalVars).toPandas(), nan_replace_value='null', figsize=[10,10] )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Single values\n",
    "  - scoring_type_code\n",
    "  - item_is_offline_scored\n",
    "- Pefect correlation\n",
    "  - is_affecting_grade & ced_assignment_type_code (previously existed)\n",
    "  - assigned_item_status & item_type_code_name (previously existed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "associations( dfFlt.filter(F.col('response_correctness').isNull() ).select(*continousVars).toPandas(), nan_replace_value='null' )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Single values\n",
    "  - assignment_attempt_number\n",
    "  - assignment_max_attempts\n",
    "- High Correlation\n",
    "  - final_score_unweighted & points_possible_unweighted"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\n",
    "\n",
    "dfFlt.filter(F.col('response_correctness').isNull() ).toPandas().plot.scatter('final_score_unweighted', 'points_possible_unweighted', title='Null', ax=ax1)\n",
    "dfFlt.sample(False, .001, 8764664).toPandas().plot.scatter('final_score_unweighted', 'points_possible_unweighted', title='Random', ax=ax2)\n",
    "plt.suptitle('response_correctness vs scores')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Null pattern similar to random"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(15,15))\n",
    "\n",
    "corr, ax1 = associations( dfFlt.filter(F.col('response_correctness').isNull() ).select(* (F.unix_timestamp(c).alias(c) for c in intervalVars) ).toPandas(), figsize=[10,10], ax=ax1 )\n",
    "\n",
    "corr, ax2 = associations( dfFlt.select(* (F.unix_timestamp(c).alias(c) for c in intervalVars) ).toPandas(), ax=ax2 )\n",
    "#plt.suptitle('response_correctness vs scores')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- New Strong correlations\n",
    "  - max and min_student_stop_datetime & assignment_due_date\n",
    "  - max and min_student_stop_datetime & assignment_final_submission_date"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Previous\n",
    "associations( dfFlt.select(* (F.unix_timestamp(c).alias(c) for c in intervalVars) ).toPandas(), figsize=[10,10] )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# null\n",
    "associations( dfFlt.filter(F.col('response_correctness').isNull() ).select('org_new', 'section_new', 'learner_new', 'assessment_new').toPandas(), nan_replace_value='null' )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Looking for single values\n",
    "  - none found"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Check Scores Associations with response_correctness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assoc_resp_cor():\n",
    "  pdDf = dfInv.select('response_correctness','final_score_unweighted').toPandas()\n",
    "  pdDf['final_score_unweighted'] = np.where(pdDf['final_score_unweighted'] == 0, 'zero', 'nonzero')\n",
    "\n",
    "  return pd.crosstab(pdDf.final_score_unweighted.fillna('null'), pdDf.response_correctness.fillna('null'))\n",
    "\n",
    "assoc_resp_cor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- response_correctness = 'correct' when score is nonzero\n",
    "- impute response_correctness based on final_score_unweighted with median\n",
    "  - final_score_unweighted is right skewed\n",
    "  -"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Median final_score_unweighted by response_correctness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def score_by_correct():\n",
    "  pdDf = dfInv.select('response_correctness','final_score_unweighted').toPandas()\n",
    "  return pdDf.groupby('response_correctness')[['final_score_unweighted']].median()\n",
    "\n",
    "score_by_correct()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# +--------------------+-----+\n",
    "# |response_correctness|count|\n",
    "# +--------------------+-----+\n",
    "# |correct             |43761|\n",
    "# |incorrect           |30496|\n",
    "# |null                |3298 |\n",
    "# |[unassigned]        |1566 |\n",
    "# |partially_correct   |1427 |\n",
    "# +--------------------+-----+\n",
    "\n",
    "pdDf = dfFlt.select('response_correctness', 'final_score_unweighted').toPandas()\n",
    "pdDf[ pdDf['response_correctness'] == 'correct']['final_score_unweighted'].median()\n",
    "pdDf[ pdDf['response_correctness'] == 'incorrect']['final_score_unweighted'].median()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Bin the data frame by \"a\" with 10 bins...\n",
    "bins = np.linspace(pdDf.final_score_unweighted.min(), pdDf.final_score_unweighted.max(), 10)\n",
    "groups = pdDf.groupby(pd.cut(pdDf.final_score_unweighted, bins))\n",
    "groups"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Display Median by response_correctness levels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "  spark.sql(\"SELECT source, percentile_approx(value, 0.5) FROM raw_data GROUP BY source\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Single values\n",
    "  - item_is_offline_scored\n",
    "  - scoring_type_code\n",
    "- Correlations\n",
    "  - is_affecting_grade & ced_assignment_type_code\n",
    "  - assigned_item_status & item_type_code_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Display Correlated Values of response_correctness = Null"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfInv.filter(F.col(\"response_correctness\").isNull() == True).select('item_is_offline_scored', 'scoring_type_code', 'is_affecting_grade', 'ced_assignment_type_code', 'assigned_item_status', 'item_type_code_name').distinct().show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explanation Null values in response_correctness Perfect Correlations\n",
    "- response_correctness is null when not scored (scoring_type_code = '[unassigned]')\n",
    "  - manually scorred\n",
    "- contridicts item_is_offline_scored = false\n",
    "  - automatically scorred\n",
    "- All perfectly correlated fields appear to be default values when not scorred"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Statistics of final_score_unweighted to response_correctness = Null"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def null_stats():\n",
    "  dfNull = dfInv.filter(F.col(\"response_correctness\").isNull() == True)\n",
    "\n",
    "  dfInv.select('final_score_unweighted').toPandas().hist(bins=60)\n",
    "  plt.suptitle('All Observations')\n",
    "  plt.title('final_score_unweighted')\n",
    "\n",
    "  dfInv.select(\"points_possible_unweighted\").toPandas().hist(bins=60)\n",
    "  plt.suptitle('All Observations')\n",
    "  plt.title('points_possible_unweighted')\n",
    "\n",
    "  dfNull.select(\"final_score_unweighted\").toPandas().hist(bins=60)\n",
    "  plt.suptitle('Null Observations')\n",
    "  plt.title('final_score_unweighted')\n",
    "\n",
    "  dfNull.select(\"points_possible_unweighted\").toPandas().hist(bins=60)\n",
    "  plt.suptitle('Null Observations')\n",
    "  plt.title('points_possible_unweighted')\n",
    "\n",
    "\n",
    "null_stats()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- closely correlated with final_score_unweighted = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "finish_todo('Investigate 3298 null values in response_correctness')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}