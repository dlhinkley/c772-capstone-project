{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Data Exploration: Summerize Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import shelve"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "%run './child/libraries.ipynb'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Load data\n",
    "shared = {}\n",
    "dfRaw  = load_df('dfRaw')\n",
    "dfDesc = load_df('dfDesc')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "%run './summary/add-id-variable-names.ipynb'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Number of observations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "149807"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfRaw.select().count()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Display Variable Descriptions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                                field            category  \\\n0                       assessment_id          Assessment   \n1      assessment_instance_attempt_id  Assignment Attempt   \n2              assessment_instance_id          Assessment   \n3         assessment_item_response_id        Item Attempt   \n4                assigned_item_status        Item Attempt   \n5           assignment_attempt_number  Assignment Attempt   \n6                 assignment_due_date          Assignment   \n7    assignment_final_submission_date          Assignment   \n8          assignment_late_submission          Assignment   \n9             assignment_max_attempts          Assignment   \n10              assignment_start_date          Assignment   \n11           ced_assignment_type_code          Assignment   \n12             final_score_unweighted  Assignment Attempt   \n13                 is_affecting_grade          Assignment   \n14                         is_deleted        Item Attempt   \n15                    is_force_scored  Assignment Attempt   \n16         is_manual_scoring_required  Assignment Attempt   \n17             item_is_offline_scored                Item   \n18                item_type_code_name                Item   \n19   learner_assigned_item_attempt_id        Item Attempt   \n20      learner_assignment_attempt_id  Assignment Attempt   \n21             learner_attempt_status  Assignment Attempt   \n22                         learner_id             Learner   \n23          max_student_stop_datetime          Assignment   \n24         min_student_start_datetime          Assignment   \n25  number_of_distinct_instance_items          Assignment   \n26                 number_of_learners          Assignment   \n27                             org_id        Organization   \n28         points_possible_unweighted  Assignment Attempt   \n29               response_correctness        Item Attempt   \n30                    scored_datetime  Assignment Attempt   \n31                  scoring_type_code                Item   \n32                         section_id             Section   \n33             student_start_datetime  Assignment Attempt   \n34              student_stop_datetime  Assignment Attempt   \n35          was_fully_scored_datetime  Assignment Attempt   \n36           was_in_progress_datetime  Assignment Attempt   \n37      was_submitted_datetime_actual  Assignment Attempt   \n\n                      type  \\\n0   Categorical Identifier   \n1   Categorical Identifier   \n2   Categorical Identifier   \n3   Categorical Identifier   \n4      Categorical Nominal   \n5       Numeric Continuous   \n6     Categorical Interval   \n7     Categorical Interval   \n8       Categorical Binary   \n9       Numeric Continuous   \n10    Categorical Interval   \n11     Categorical Nominal   \n12      Numeric Continuous   \n13      Categorical Binary   \n14      Categorical Binary   \n15      Categorical Binary   \n16      Categorical Binary   \n17      Categorical Binary   \n18     Categorical Nominal   \n19  Categorical Identifier   \n20  Categorical Identifier   \n21     Categorical Nominal   \n22  Categorical Identifier   \n23    Categorical Interval   \n24    Categorical Interval   \n25      Numeric Continuous   \n26      Numeric Continuous   \n27  Categorical Identifier   \n28      Numeric Continuous   \n29     Categorical Nominal   \n30    Categorical Interval   \n31     Categorical Nominal   \n32  Categorical Identifier   \n33    Categorical Interval   \n34    Categorical Interval   \n35    Categorical Interval   \n36    Categorical Interval   \n37    Categorical Interval   \n\n                                                                                                                                                                                                       description  \n0                                                                                                                                                                                             ID of an Assessment.  \n1                                                                                                                                                                      ID of a learner's attempt of an assessment.  \n2                                                                                                                                                                     ID of a section's instance of an assessment.  \n3                                                                                                                                                                 ID of a learners response to an assessment item.  \n4                                                          Code indicating the status of the assessment item.  Values: scored, offline_scored: manually scored, assigned: not yet started, responded: being scored  \n5                                                                                                                                                        The nth time a certain learner attempted the assessement.  \n6                                                                                                                                                                         The date and time the assignment is due.  \n7                                                                                                                                                      The date and time the assignment was submitted for scoring.  \n8                                                                                                                                                                       True if the assignment was submitted late.  \n9                                                                                                                                                        The number of times a learner can attempt the assessment.  \n10                                                                                                                                               The date and time a section is scheduled to start the assignment.  \n11                                                                                                                                             Type of assessment values: rubric, assessment, practice, clo, game.  \n12                                                                                                                                                                          Numeric final score of the assessment.  \n13                                                                                                                                                        True if the assessment item affects the learner's grade.  \n14                                                                                                                                                                                  True if the record is deleted.  \n15  True if the assessment attempt was forced to be scored because the learner did not submit the assignment before the final possible submission datetime and the policy dictates forced scoring for that format.  \n16                                                                                                       True if the assessment attempt requires a manual score. Changes to false after manual scoring is entered.  \n17                                                                                                                                      True if the assessment item was scored manually. Example: Essay questions.  \n18                                                                                                                              Code indicating the type of assessment item.  Example: shortAnswer, matching, etc.  \n19                                                                                                                                                                ID of and assessment item assigned to a learner.  \n20                                                                                                                                     ID of one of possibly several attempts by a learner to pass the assessment.  \n21                                                                                           The status of a learner's attempt of the assessment.  Example: assigned, awaiting outcome, fully scored, in progress.  \n22                                                                                                                                                                                                ID of a learner.  \n23                                                                                                                                                  The latest date and time a learner must finish the assessment.  \n24                                                                                                                                                  The earliest date and time a learner can start the assessment.  \n25                                                                                                                                                                              Number of questions on assessment.  \n26                                                                                                          The number of learner's assigned this instance of an assessement. The number of learners in a section.  \n27                                                                                                                                                              ID of a organization containing multiple sections.  \n28                                                                                                                                                               Numeric highest possible score of the assessment.  \n29                                                                                                   Code indicating the correctness of the assessment item.  Example: incorrect, correct, partially_correct, etc.  \n30                                                                                                                                                            The date and time the assessment attempt was scored.  \n31                                                    Code indicating the type of scoring for the assessment item.  Values: external and  automatic: scored by computer; manual and unassiged: manually by a human  \n32                                                                                                                                                                   ID of a section containing multiple students.  \n33                                                                                                                                             The date and time the learner began the attempt of this assessment.  \n34                                                                                                                                          The date and time the learner finished the attempt of this assessment.  \n35                                                                                                                                                                    The date and time the assessment was scored.  \n36                                                                                                                                                       The date and time the assessment attempt was in progress.  \n37                                                                                                                                                     The date and time the assessment was submitted for scoring.  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>field</th>\n      <th>category</th>\n      <th>type</th>\n      <th>description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>assessment_id</td>\n      <td>Assessment</td>\n      <td>Categorical Identifier</td>\n      <td>ID of an Assessment.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>assessment_instance_attempt_id</td>\n      <td>Assignment Attempt</td>\n      <td>Categorical Identifier</td>\n      <td>ID of a learner's attempt of an assessment.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>assessment_instance_id</td>\n      <td>Assessment</td>\n      <td>Categorical Identifier</td>\n      <td>ID of a section's instance of an assessment.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>assessment_item_response_id</td>\n      <td>Item Attempt</td>\n      <td>Categorical Identifier</td>\n      <td>ID of a learners response to an assessment item.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>assigned_item_status</td>\n      <td>Item Attempt</td>\n      <td>Categorical Nominal</td>\n      <td>Code indicating the status of the assessment item.  Values: scored, offline_scored: manually scored, assigned: not yet started, responded: being scored</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>assignment_attempt_number</td>\n      <td>Assignment Attempt</td>\n      <td>Numeric Continuous</td>\n      <td>The nth time a certain learner attempted the assessement.</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>assignment_due_date</td>\n      <td>Assignment</td>\n      <td>Categorical Interval</td>\n      <td>The date and time the assignment is due.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>assignment_final_submission_date</td>\n      <td>Assignment</td>\n      <td>Categorical Interval</td>\n      <td>The date and time the assignment was submitted for scoring.</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>assignment_late_submission</td>\n      <td>Assignment</td>\n      <td>Categorical Binary</td>\n      <td>True if the assignment was submitted late.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>assignment_max_attempts</td>\n      <td>Assignment</td>\n      <td>Numeric Continuous</td>\n      <td>The number of times a learner can attempt the assessment.</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>assignment_start_date</td>\n      <td>Assignment</td>\n      <td>Categorical Interval</td>\n      <td>The date and time a section is scheduled to start the assignment.</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>ced_assignment_type_code</td>\n      <td>Assignment</td>\n      <td>Categorical Nominal</td>\n      <td>Type of assessment values: rubric, assessment, practice, clo, game.</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>final_score_unweighted</td>\n      <td>Assignment Attempt</td>\n      <td>Numeric Continuous</td>\n      <td>Numeric final score of the assessment.</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>is_affecting_grade</td>\n      <td>Assignment</td>\n      <td>Categorical Binary</td>\n      <td>True if the assessment item affects the learner's grade.</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>is_deleted</td>\n      <td>Item Attempt</td>\n      <td>Categorical Binary</td>\n      <td>True if the record is deleted.</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>is_force_scored</td>\n      <td>Assignment Attempt</td>\n      <td>Categorical Binary</td>\n      <td>True if the assessment attempt was forced to be scored because the learner did not submit the assignment before the final possible submission datetime and the policy dictates forced scoring for that format.</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>is_manual_scoring_required</td>\n      <td>Assignment Attempt</td>\n      <td>Categorical Binary</td>\n      <td>True if the assessment attempt requires a manual score. Changes to false after manual scoring is entered.</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>item_is_offline_scored</td>\n      <td>Item</td>\n      <td>Categorical Binary</td>\n      <td>True if the assessment item was scored manually. Example: Essay questions.</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>item_type_code_name</td>\n      <td>Item</td>\n      <td>Categorical Nominal</td>\n      <td>Code indicating the type of assessment item.  Example: shortAnswer, matching, etc.</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>learner_assigned_item_attempt_id</td>\n      <td>Item Attempt</td>\n      <td>Categorical Identifier</td>\n      <td>ID of and assessment item assigned to a learner.</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>learner_assignment_attempt_id</td>\n      <td>Assignment Attempt</td>\n      <td>Categorical Identifier</td>\n      <td>ID of one of possibly several attempts by a learner to pass the assessment.</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>learner_attempt_status</td>\n      <td>Assignment Attempt</td>\n      <td>Categorical Nominal</td>\n      <td>The status of a learner's attempt of the assessment.  Example: assigned, awaiting outcome, fully scored, in progress.</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>learner_id</td>\n      <td>Learner</td>\n      <td>Categorical Identifier</td>\n      <td>ID of a learner.</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>max_student_stop_datetime</td>\n      <td>Assignment</td>\n      <td>Categorical Interval</td>\n      <td>The latest date and time a learner must finish the assessment.</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>min_student_start_datetime</td>\n      <td>Assignment</td>\n      <td>Categorical Interval</td>\n      <td>The earliest date and time a learner can start the assessment.</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>number_of_distinct_instance_items</td>\n      <td>Assignment</td>\n      <td>Numeric Continuous</td>\n      <td>Number of questions on assessment.</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>number_of_learners</td>\n      <td>Assignment</td>\n      <td>Numeric Continuous</td>\n      <td>The number of learner's assigned this instance of an assessement. The number of learners in a section.</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>org_id</td>\n      <td>Organization</td>\n      <td>Categorical Identifier</td>\n      <td>ID of a organization containing multiple sections.</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>points_possible_unweighted</td>\n      <td>Assignment Attempt</td>\n      <td>Numeric Continuous</td>\n      <td>Numeric highest possible score of the assessment.</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>response_correctness</td>\n      <td>Item Attempt</td>\n      <td>Categorical Nominal</td>\n      <td>Code indicating the correctness of the assessment item.  Example: incorrect, correct, partially_correct, etc.</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>scored_datetime</td>\n      <td>Assignment Attempt</td>\n      <td>Categorical Interval</td>\n      <td>The date and time the assessment attempt was scored.</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>scoring_type_code</td>\n      <td>Item</td>\n      <td>Categorical Nominal</td>\n      <td>Code indicating the type of scoring for the assessment item.  Values: external and  automatic: scored by computer; manual and unassiged: manually by a human</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>section_id</td>\n      <td>Section</td>\n      <td>Categorical Identifier</td>\n      <td>ID of a section containing multiple students.</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>student_start_datetime</td>\n      <td>Assignment Attempt</td>\n      <td>Categorical Interval</td>\n      <td>The date and time the learner began the attempt of this assessment.</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>student_stop_datetime</td>\n      <td>Assignment Attempt</td>\n      <td>Categorical Interval</td>\n      <td>The date and time the learner finished the attempt of this assessment.</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>was_fully_scored_datetime</td>\n      <td>Assignment Attempt</td>\n      <td>Categorical Interval</td>\n      <td>The date and time the assessment was scored.</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>was_in_progress_datetime</td>\n      <td>Assignment Attempt</td>\n      <td>Categorical Interval</td>\n      <td>The date and time the assessment attempt was in progress.</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>was_submitted_datetime_actual</td>\n      <td>Assignment Attempt</td>\n      <td>Categorical Interval</td>\n      <td>The date and time the assessment was submitted for scoring.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "dfPd = dfDesc.toPandas()\n",
    "dfPd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Display Variable Categories"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|category          |\n",
      "+------------------+\n",
      "|Section           |\n",
      "|Assignment Attempt|\n",
      "|Item              |\n",
      "|Assessment        |\n",
      "|Learner           |\n",
      "|Organization      |\n",
      "|Item Attempt      |\n",
      "|Assignment        |\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDesc.select('category').distinct().show(20,False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save Variable Categories"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "shared['orgVars']               = dfPd.loc[ dfPd['category'] == 'Organization' ].field.tolist()\n",
    "shared['sectionVars']           = dfPd.loc[ dfPd['category'] == 'Section' ].field.tolist()\n",
    "shared['learnerVars']           = dfPd.loc[ dfPd['category'] == 'Learner' ].field.tolist()\n",
    "shared['assessmentVars']        = dfPd.loc[ dfPd['category'] == 'Assessment' ].field.tolist()\n",
    "shared['assignmentVars']        = dfPd.loc[ dfPd['category'] == 'Assignment' ].field.tolist()\n",
    "shared['itemVars']              = dfPd.loc[ dfPd['category'] == 'Item' ].field.tolist()\n",
    "shared['assignmentAttemptVars'] = dfPd.loc[ dfPd['category'] == 'Assignment Attempt' ].field.tolist()\n",
    "shared['itemAttemptVars']       = dfPd.loc[ dfPd['category'] == 'Item Attempt' ].field.tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "%run './summary/display-sample-data.ipynb'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|Categorical Identifier          |\n",
      "+--------------------------------+\n",
      "|assessment_id                   |\n",
      "|assessment_instance_attempt_id  |\n",
      "|assessment_instance_id          |\n",
      "|assessment_item_response_id     |\n",
      "|learner_assigned_item_attempt_id|\n",
      "|learner_assignment_attempt_id   |\n",
      "|learner_id                      |\n",
      "|org_id                          |\n",
      "|section_id                      |\n",
      "+--------------------------------+\n",
      "\n",
      "+------------------------+\n",
      "|Categorical Nominal     |\n",
      "+------------------------+\n",
      "|assigned_item_status    |\n",
      "|ced_assignment_type_code|\n",
      "|item_type_code_name     |\n",
      "|learner_attempt_status  |\n",
      "|response_correctness    |\n",
      "|scoring_type_code       |\n",
      "+------------------------+\n",
      "\n",
      "+---------------------------------+\n",
      "|Numeric Continuous               |\n",
      "+---------------------------------+\n",
      "|assignment_attempt_number        |\n",
      "|assignment_max_attempts          |\n",
      "|final_score_unweighted           |\n",
      "|number_of_distinct_instance_items|\n",
      "|number_of_learners               |\n",
      "|points_possible_unweighted       |\n",
      "+---------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|Categorical Interval            |\n",
      "+--------------------------------+\n",
      "|assignment_due_date             |\n",
      "|assignment_final_submission_date|\n",
      "|assignment_start_date           |\n",
      "|max_student_stop_datetime       |\n",
      "|min_student_start_datetime      |\n",
      "|scored_datetime                 |\n",
      "|student_start_datetime          |\n",
      "|student_stop_datetime           |\n",
      "|was_fully_scored_datetime       |\n",
      "|was_in_progress_datetime        |\n",
      "|was_submitted_datetime_actual   |\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------------------+\n",
      "|Categorical Binary        |\n",
      "+--------------------------+\n",
      "|assignment_late_submission|\n",
      "|is_affecting_grade        |\n",
      "|is_deleted                |\n",
      "|is_force_scored           |\n",
      "|is_manual_scoring_required|\n",
      "|item_is_offline_scored    |\n",
      "+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run './summary/display-variables.ipynb'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Change Date Fields from String to Timestamp Type"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "for f in shared['intervalVars']:\n",
    "  dfRaw = dfRaw.withColumn(f, F.col(f).cast(T.TimestampType() ) )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Display Schema"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ced_assignment_type_code: string (nullable = true)\n",
      " |-- is_affecting_grade: boolean (nullable = true)\n",
      " |-- number_of_learners: integer (nullable = true)\n",
      " |-- number_of_distinct_instance_items: integer (nullable = true)\n",
      " |-- assignment_max_attempts: integer (nullable = true)\n",
      " |-- assignment_late_submission: boolean (nullable = true)\n",
      " |-- assignment_final_submission_date: timestamp (nullable = true)\n",
      " |-- assignment_start_date: timestamp (nullable = true)\n",
      " |-- assignment_due_date: timestamp (nullable = true)\n",
      " |-- min_student_start_datetime: timestamp (nullable = true)\n",
      " |-- max_student_stop_datetime: timestamp (nullable = true)\n",
      " |-- assignment_attempt_number: integer (nullable = true)\n",
      " |-- was_fully_scored_datetime: timestamp (nullable = true)\n",
      " |-- was_submitted_datetime_actual: timestamp (nullable = true)\n",
      " |-- was_in_progress_datetime: timestamp (nullable = true)\n",
      " |-- is_force_scored: boolean (nullable = true)\n",
      " |-- is_manual_scoring_required: boolean (nullable = true)\n",
      " |-- student_start_datetime: timestamp (nullable = true)\n",
      " |-- student_stop_datetime: timestamp (nullable = true)\n",
      " |-- item_is_offline_scored: boolean (nullable = true)\n",
      " |-- learner_attempt_status: string (nullable = true)\n",
      " |-- points_possible_unweighted: double (nullable = true)\n",
      " |-- final_score_unweighted: integer (nullable = true)\n",
      " |-- scored_datetime: timestamp (nullable = true)\n",
      " |-- item_type_code_name: string (nullable = true)\n",
      " |-- scoring_type_code: string (nullable = true)\n",
      " |-- response_correctness: string (nullable = true)\n",
      " |-- assigned_item_status: string (nullable = true)\n",
      " |-- is_deleted: boolean (nullable = true)\n",
      " |-- org_id: integer (nullable = true)\n",
      " |-- section_id: integer (nullable = true)\n",
      " |-- assessment_id: integer (nullable = true)\n",
      " |-- assessment_instance_id: integer (nullable = true)\n",
      " |-- learner_assignment_attempt_id: integer (nullable = true)\n",
      " |-- assessment_instance_attempt_id: integer (nullable = true)\n",
      " |-- learner_id: integer (nullable = true)\n",
      " |-- learner_assigned_item_attempt_id: integer (nullable = true)\n",
      " |-- assessment_item_response_id: integer (nullable = true)\n",
      " |-- org_new: string (nullable = true)\n",
      " |-- learner_new: string (nullable = true)\n",
      " |-- section_new: string (nullable = true)\n",
      " |-- assessment_new: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfRaw.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Only Summarize \"fully scored\" items"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Filter to learner_attempt_status = 'fully scored'\n",
    "dfFlt = dfRaw.filter(F.col('learner_attempt_status') == 'fully scored')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assignment_due_date = 1566\n",
      "assignment_final_submission_date = 1566\n",
      "assignment_start_date = 1566\n",
      "scored_datetime = 3422\n",
      "student_start_datetime = 749\n",
      "student_stop_datetime = 749\n",
      "was_fully_scored_datetime = 750\n",
      "was_in_progress_datetime = 9965\n",
      "was_submitted_datetime_actual = 18469\n"
     ]
    }
   ],
   "source": [
    "%run './summary/convert-default-dates.ipynb'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assessment_id\n",
      "+------+----+\n",
      "|unique|null|\n",
      "+------+----+\n",
      "|   329|   0|\n",
      "+------+----+\n",
      "\n",
      "assessment_instance_attempt_id\n",
      "+------+----+\n",
      "|unique|null|\n",
      "+------+----+\n",
      "|  8483|3264|\n",
      "+------+----+\n",
      "\n",
      "assessment_instance_id\n",
      "+------+----+\n",
      "|unique|null|\n",
      "+------+----+\n",
      "|   615|   0|\n",
      "+------+----+\n",
      "\n",
      "assessment_item_response_id\n",
      "+------+-----+\n",
      "|unique| null|\n",
      "+------+-----+\n",
      "| 64368|15710|\n",
      "+------+-----+\n",
      "\n",
      "learner_assigned_item_attempt_id\n",
      "+------+----+\n",
      "|unique|null|\n",
      "+------+----+\n",
      "| 79689|   0|\n",
      "+------+----+\n",
      "\n",
      "learner_assignment_attempt_id\n",
      "+------+----+\n",
      "|unique|null|\n",
      "+------+----+\n",
      "|  8855|   0|\n",
      "+------+----+\n",
      "\n",
      "learner_id\n",
      "+------+----+\n",
      "|unique|null|\n",
      "+------+----+\n",
      "|  1126|   0|\n",
      "+------+----+\n",
      "\n",
      "org_id\n",
      "+------+----+\n",
      "|unique|null|\n",
      "+------+----+\n",
      "|     3|   0|\n",
      "+------+----+\n",
      "\n",
      "section_id\n",
      "+------+----+\n",
      "|unique|null|\n",
      "+------+----+\n",
      "|    46|   0|\n",
      "+------+----+\n",
      "\n",
      "+--------------------+-----+\n",
      "|assigned_item_status|count|\n",
      "+--------------------+-----+\n",
      "|scored              |64804|\n",
      "|offline_scored      |12446|\n",
      "|assigned            |3264 |\n",
      "|responded           |34   |\n",
      "+--------------------+-----+\n",
      "\n",
      "+------------------------+-----+\n",
      "|ced_assignment_type_code|count|\n",
      "+------------------------+-----+\n",
      "|assessment              |76172|\n",
      "|practice                |2562 |\n",
      "|game                    |1555 |\n",
      "|clo                     |248  |\n",
      "|rubric                  |11   |\n",
      "+------------------------+-----+\n",
      "\n",
      "+----------------------+-----+\n",
      "|item_type_code_name   |count|\n",
      "+----------------------+-----+\n",
      "|multipleChoice        |32451|\n",
      "|fillInTheBlank        |12072|\n",
      "|equationEntry         |9516 |\n",
      "|trueFalse             |4963 |\n",
      "|cloze                 |3576 |\n",
      "|null                  |3264 |\n",
      "|multipleSelect        |2691 |\n",
      "|graphing              |2415 |\n",
      "|MultipleChoiceResponse|1550 |\n",
      "|choiceMatrix          |1336 |\n",
      "|matching              |1164 |\n",
      "|bucketing             |1051 |\n",
      "|selectText            |1025 |\n",
      "|essay                 |1000 |\n",
      "|shortAnswer           |983  |\n",
      "|sortable              |644  |\n",
      "|numberLine            |455  |\n",
      "|aheAlgo               |219  |\n",
      "|imageLabel            |108  |\n",
      "|fileUpload            |49   |\n",
      "|RubricResponse        |11   |\n",
      "|FillinBlankResponse   |5    |\n",
      "+----------------------+-----+\n",
      "\n",
      "+----------------------+-----+\n",
      "|learner_attempt_status|count|\n",
      "+----------------------+-----+\n",
      "|fully scored          |80548|\n",
      "+----------------------+-----+\n",
      "\n",
      "+--------------------+-----+\n",
      "|response_correctness|count|\n",
      "+--------------------+-----+\n",
      "|correct             |43761|\n",
      "|incorrect           |30496|\n",
      "|null                |3298 |\n",
      "|[unassigned]        |1566 |\n",
      "|partially_correct   |1427 |\n",
      "+--------------------+-----+\n",
      "\n",
      "+-----------------+-----+\n",
      "|scoring_type_code|count|\n",
      "+-----------------+-----+\n",
      "|automatic        |73467|\n",
      "|[unassigned]     |4864 |\n",
      "|manual           |1998 |\n",
      "|external         |219  |\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4533.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:848)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 134.0 failed 1 times, most recent failure: Lost task 0.0 in stage 134.0 (TID 7539, duanes-macbook.lan, executor driver): java.io.FileNotFoundException: File file:/Users/duane.hinkley/PycharmProjects/c772-capstone-project/juniper/.data/todoList.parquet/part-00000-194e4afb-6e44-44ac-b5c9-15ed53f9d839-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:491)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 33 more\nCaused by: java.io.FileNotFoundException: File file:/Users/duane.hinkley/PycharmProjects/c772-capstone-project/juniper/.data/todoList.parquet/part-00000-194e4afb-6e44-44ac-b5c9-15ed53f9d839-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:491)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/juniper/summary/descriptive-statistics.ipynb\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Create Todo list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0madd_todo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Investigate 3298 null values in response_correctness'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0madd_todo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Investigate 1566 [unassigned] in response_correctness'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0madd_todo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Investigate null values in item_type_code_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0madd_todo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Reduce number of levels in item_type_code_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/juniper/summary/descriptive-statistics.ipynb\u001B[0m in \u001B[0;36madd_todo\u001B[0;34m(desc)\u001B[0m\n\u001B[1;32m     16\u001B[0m       \u001B[0mnewRow\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdesc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m       \u001B[0mtodoList\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtodoList\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnewRow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m       \u001B[0msave_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtodoList\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'todoList'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mlist_todo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/juniper/summary/descriptive-statistics.ipynb\u001B[0m in \u001B[0;36msave_df\u001B[0;34m(df, name)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0msave_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m     \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrepartition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'overwrite'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\".data/\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mname\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\".parquet\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0msave_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mparquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m    934\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    935\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_set_opts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcompression\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcompression\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 936\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    937\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    938\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0msince\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1.6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1304\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1305\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1307\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    126\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 128\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    129\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o4533.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:848)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 134.0 failed 1 times, most recent failure: Lost task 0.0 in stage 134.0 (TID 7539, duanes-macbook.lan, executor driver): java.io.FileNotFoundException: File file:/Users/duane.hinkley/PycharmProjects/c772-capstone-project/juniper/.data/todoList.parquet/part-00000-194e4afb-6e44-44ac-b5c9-15ed53f9d839-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:491)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 33 more\nCaused by: java.io.FileNotFoundException: File file:/Users/duane.hinkley/PycharmProjects/c772-capstone-project/juniper/.data/todoList.parquet/part-00000-194e4afb-6e44-44ac-b5c9-15ed53f9d839-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:491)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4533.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:848)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 134.0 failed 1 times, most recent failure: Lost task 0.0 in stage 134.0 (TID 7539, duanes-macbook.lan, executor driver): java.io.FileNotFoundException: File file:/Users/duane.hinkley/PycharmProjects/c772-capstone-project/juniper/.data/todoList.parquet/part-00000-194e4afb-6e44-44ac-b5c9-15ed53f9d839-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:491)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 33 more\nCaused by: java.io.FileNotFoundException: File file:/Users/duane.hinkley/PycharmProjects/c772-capstone-project/juniper/.data/todoList.parquet/part-00000-194e4afb-6e44-44ac-b5c9-15ed53f9d839-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:491)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-17-6d710b9b3c55>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'run'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"'./summary/descriptive-statistics.ipynb'\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mrun_line_magic\u001B[0;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[1;32m   2324\u001B[0m                 \u001B[0mkwargs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'local_ns'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msys\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_getframe\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstack_depth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf_locals\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2325\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbuiltin_trap\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2326\u001B[0;31m                 \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2327\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2328\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<decorator-gen-59>\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, parameter_s, runner, file_finder)\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/IPython/core/magic.py\u001B[0m in \u001B[0;36m<lambda>\u001B[0;34m(f, *a, **k)\u001B[0m\n\u001B[1;32m    185\u001B[0m     \u001B[0;31m# but it's overkill for just that one bit of state.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    186\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mmagic_deco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 187\u001B[0;31m         \u001B[0mcall\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mlambda\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    188\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    189\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mcallable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/IPython/core/magics/execution.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, parameter_s, runner, file_finder)\u001B[0m\n\u001B[1;32m    716\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mpreserve_keys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshell\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muser_ns\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'__file__'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    717\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshell\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muser_ns\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'__file__'\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfilename\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 718\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshell\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msafe_execfile_ipy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mraise_exceptions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    719\u001B[0m             \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    720\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36msafe_execfile_ipy\u001B[0;34m(self, fname, shell_futures, raise_exceptions)\u001B[0m\n\u001B[1;32m   2801\u001B[0m                     \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_cell\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcell\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msilent\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mshell_futures\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mshell_futures\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2802\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mraise_exceptions\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2803\u001B[0;31m                         \u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_error\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2804\u001B[0m                     \u001B[0;32melif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mresult\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msuccess\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2805\u001B[0m                         \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001B[0m in \u001B[0;36mraise_error\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    329\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merror_before_exec\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merror_in_exec\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 331\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merror_in_exec\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    332\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    333\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__repr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "    \u001B[0;31m[... skipping hidden 1 frame]\u001B[0m\n",
      "\u001B[0;32m<ipython-input-17-1904b328fa4c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Create Todo list\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0madd_todo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Investigate 3298 null values in response_correctness'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0madd_todo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Investigate 1566 [unassigned] in response_correctness'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0madd_todo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Investigate null values in item_type_code_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0madd_todo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Reduce number of levels in item_type_code_name'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-26d68ac59e67>\u001B[0m in \u001B[0;36madd_todo\u001B[0;34m(desc)\u001B[0m\n\u001B[1;32m     16\u001B[0m       \u001B[0mnewRow\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mspark\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcreateDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdesc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m       \u001B[0mtodoList\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtodoList\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnewRow\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m       \u001B[0msave_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtodoList\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'todoList'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mlist_todo\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-41f5b5ceb177>\u001B[0m in \u001B[0;36msave_df\u001B[0;34m(df, name)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0msave_df\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 8\u001B[0;31m     \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrepartition\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'overwrite'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\".data/\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mname\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\".parquet\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0msave_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001B[0m in \u001B[0;36mparquet\u001B[0;34m(self, path, mode, partitionBy, compression)\u001B[0m\n\u001B[1;32m    934\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpartitionBy\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    935\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_set_opts\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcompression\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcompression\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 936\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jwrite\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparquet\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    937\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    938\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0msince\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1.6\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1303\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1304\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1305\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1306\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1307\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    126\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    127\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 128\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    129\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    130\u001B[0m             \u001B[0mconverted\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconvert_exception\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjava_exception\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/c772-capstone-project/venv/lib/python3.6/site-packages/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    326\u001B[0m                 raise Py4JJavaError(\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    330\u001B[0m                 raise Py4JError(\n",
      "\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o4533.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:178)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:108)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:106)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:122)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:121)\n\tat org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:963)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:399)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:288)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:848)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 134.0 failed 1 times, most recent failure: Lost task 0.0 in stage 134.0 (TID 7539, duanes-macbook.lan, executor driver): java.io.FileNotFoundException: File file:/Users/duane.hinkley/PycharmProjects/c772-capstone-project/juniper/.data/todoList.parquet/part-00000-194e4afb-6e44-44ac-b5c9-15ed53f9d839-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:491)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:844)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:195)\n\t... 33 more\nCaused by: java.io.FileNotFoundException: File file:/Users/duane.hinkley/PycharmProjects/c772-capstone-project/juniper/.data/todoList.parquet/part-00000-194e4afb-6e44-44ac-b5c9-15ed53f9d839-c000.snappy.parquet does not exist\nIt is possible the underlying files have been updated. You can explicitly invalidate the cache in Spark by running 'REFRESH TABLE tableName' command in SQL or by recreating the Dataset/DataFrame involved.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:169)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:93)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:491)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1135)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "%run './summary/descriptive-statistics.ipynb'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save Work\n",
    "save_dict(shared, 'shared')\n",
    "save_df(dfFlt, 'dfFlt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "name": "C772 Capstone Summarization Sub-Notebook",
  "notebookId": 199670002753040
 },
 "nbformat": 4,
 "nbformat_minor": 1
}