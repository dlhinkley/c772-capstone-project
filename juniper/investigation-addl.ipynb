{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_todo()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Investigate null values in item_type_code_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Cross Tabulation of scoring_type_code and response_correctness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pdDf = dfInv.toPandas()\n",
    "pd.crosstab(pdDf.learner_attempt_status.fillna('null'), pdDf.response_correctness.fillna('null'),)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def testing():\n",
    "  cols = ['assigned_item_status', 'ced_assignment_type_code', 'is_affecting_grade', 'is_force_scored', 'item_is_offline_scored', 'item_type_code_name',  'response_correctness', 'scoring_type_code']\n",
    "  pdDf = dfInv.select(*cols).toPandas()\n",
    "  # Plot features associations\n",
    "  associations(pdDf)\n",
    "\n",
    "testing()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Cross Tabulation of"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pdDf = dfInv.toPandas()\n",
    "pd.crosstab(pdDf.assigned_item_status.fillna('null'),     pdDf.item_is_offline_scored.fillna('null'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Cross Tabulation of"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.crosstab(pdDf.ced_assignment_type_code.fillna('null'), pdDf.is_affecting_grade.fillna('null'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Cross Tabulation of"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.crosstab(pdDf.item_type_code_name.fillna('null'), pdDf.scoring_type_code.fillna('null'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Can final_score_unweighted be used to predict response_correctness?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Count the number of each response_correctness level where the final_score_unweighted is 0\n",
    "dfInv.filter(col(\"final_score_unweighted\") == 0).groupBy(\"response_correctness\").count().orderBy(\"count\", ascending=False).show(50, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- final_score_unweighted be used to predict response_correctness\n",
    "  - no\n",
    "- is response_correctness null or incorrect if final_score_unweighted is 0\n",
    "  - no"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Are the following related?\n",
    "- assignment_due_date, assignment_final_submission_date and assignment_start_date have 1566 empty dates\n",
    "- assignment_attempt_number and assignment_max_attempts have 1566 records with the value 0\n",
    "- there are 1566 records with the response_correctness \"[unassigned]\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Cross-tabulation learner_attempt_status vs response_correctness"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfPd = dfInv.toPandas()\n",
    "# Return cross-tabulation table of learner_attempt_status vs response_correctness adding counts for null values\n",
    "pd.crosstab(dfPd.learner_attempt_status.fillna('null'), dfPd.response_correctness.fillna('null'), margins=True, margins_name=\"Total\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Cross-tabulation learner_attempt_status vs assigned_item_status"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Return cross-tabulation table of learner_attempt_status vs assigned_item_status adding counts for null values\n",
    "pd.crosstab(dfPd.learner_attempt_status.fillna('null'), dfPd.assigned_item_status.fillna('null'), margins=True, margins_name=\"Total\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Cross-tabulation learner_attempt_status vs scoring_type_code"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Return cross-tabulation table of learner_attempt_status vs scoring_type_code adding counts for null values\n",
    "pd.crosstab(dfPd.learner_attempt_status.fillna('null'), dfPd.scoring_type_code.fillna('null'), margins=True, margins_name=\"Total\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Interval Correlations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, col\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dfPd =  filter_default(dfInv).select(* (unix_timestamp(c).alias(c) for c in intervalVars) ).toPandas()\n",
    "\n",
    "corrMatrix = dfPd.corr()\n",
    "plt.figure(figsize=(10,12))\n",
    "sn.heatmap(corrMatrix, annot=True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interval Correlation Results\n",
    "- Perfectly correlated (1 or -1)\n",
    "  - assignment_due_date and..\n",
    "    - assignment_final_submission_date\n",
    "    - assignment_start_date\n",
    "    - Comments: Verify and possibly use only one\n",
    "- Highly correlated (> .7)\n",
    "  - max_student_start_datetime and min_student_stop_datetime\n",
    "    - Explanation: quizes are short and ending always follows starting\n",
    "  - scored_datetime and..\n",
    "    - student_stop_datetime\n",
    "    - was_fully_scored_datetime\n",
    "    - was_submitted_datetime_actual\n",
    "    - Explanation: actions closely follow stopping\n",
    "  - student_start_datetime and..\n",
    "    - student_stop_datetime\n",
    "    - was_inprogress_datetime\n",
    "    - Explanation: quizes are short and ending always follows starting\n",
    "  - student_stop_datetime and..\n",
    "    - scored_datetime\n",
    "    - student_start_datetime\n",
    "    - was_fully_scored_datetime\n",
    "    - Explanation: quizes are short and ending always follows starting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "date_stats(dfInv, \"assignment_due_date\", \"assignment_final_submission_date\").describe().show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Investigations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Is learner_attempt_status of \"fully scored\" perfectly correlated other variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cols = single_val(dfClean, \"learner_attempt_status\")\n",
    "print(\"Correlated Vars\", cols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- No"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = dfClean.filter(col(\"assignment_due_date\").isNull() == True)\n",
    "cols = single_val(df, \"assignment_due_date\")\n",
    "\n",
    "# Show the results on two lines\n",
    "length = len(cols)\n",
    "middle_index = length//2\n",
    "first_half = cols[:middle_index]\n",
    "second_half = cols[middle_index:]\n",
    "\n",
    "df.select(*first_half).show(1)\n",
    "df.select(*second_half).show(1)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Yes they are all related\n",
    "- only occurs in one organization\n",
    "- could be record keeping differences\n",
    "- EXCLUDE?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Why Are Empty Dates Empty"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "# Count Default / Empty Dates\n",
    "\n",
    "for f in intervalVars:\n",
    "  count = dfClean.filter(col(f).isNull() == True).count()\n",
    "  print (f,\"=\", count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = dfClean.filter(col(\"scored_datetime\").isNull() == True)\n",
    "cols = single_val(df, \"scored_datetime\")\n",
    "df.select(*cols).show(1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "scored_datetime is null at times when fully scored"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "df = dfClean.filter(col(\"student_start_datetime\").isNull() == True)\n",
    "cols = single_val(df, \"student_start_datetime\")\n",
    "\n",
    "# Show the results on two lines\n",
    "\n",
    "df.select(*cols[:5]).show(1)\n",
    "df.select(*cols[5:9]).show(1)\n",
    "df.select(*cols[9:]).show(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Null for one organization\n",
    "- Could just be record keeping difference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = dfClean.filter(col(\"was_in_progress_datetime\").isNull() == True)\n",
    "cols = single_val(df, \"was_in_progress_datetime\")\n",
    "df.select(*cols).show(1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Only when fully scored"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = dfClean.filter(col(\"was_submitted_datetime_actual\").isNull() == True)\n",
    "cols = single_val(df, \"was_submitted_datetime_actual\")\n",
    "df.select(*cols).show(1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Only when fully scored"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reclassify categories in item_type_code_name"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Before Categories\n",
    "dfClean.select(\"item_type_code_name\").distinct().orderBy(\"item_type_code_name\").show(50, False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Combine Suffix Levels\n",
    "- The levels with the suffix Response (ex: FillinBlankResponse) is the same type of question as level without the suffix (ex: fillInTheBlank)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Combine fillInTheBlank and FillinBlankResponse\n",
    "dfClean = dfClean.withColumn(\"item_type_code_name\", when( col(\"item_type_code_name\") == \"FillinBlankResponse\", \"fillInTheBlank\" ).otherwise(col(\"item_type_code_name\")) )\n",
    "\n",
    "# Combine multipleChoice and MultipleChoiceResponse\n",
    "dfClean = dfClean.withColumn(\"item_type_code_name\", when( col(\"item_type_code_name\") == \"MultipleChoiceResponse\", \"multipleChoice\" ).otherwise(col(\"item_type_code_name\")) )\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%sql\n",
    "select assessment_instance_id,\n",
    " section_id,\n",
    " assessment_instance_attempt_id,\n",
    " assignment_start_date\n",
    "FROM clean_data\n",
    "WHERE assessment_instance_attempt_id IS NOT NULL\n",
    "ORDER BY assessment_instance_id, assessment_instance_attempt_id, student_start_datetime LIMIT 100\n",
    ";"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The assignment start date is the same for every assessment_instance_id.  Looks like it's assigned with the instance is created"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM clean_data WHERE assignment_start_date IS NULL;"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hmm... there's that 1566 again.  Let's look at those records"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dfDesc.select('field').show(40, False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_dict(shared, 'shared2')\n",
    "save_dict(dfInv, 'dfInv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}