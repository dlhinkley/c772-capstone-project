{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Temp for local development\n",
    "import ssl\n",
    "import os\n",
    "# os.environ['PYSPARK_PYTHON'] = '/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6'\n",
    "import tinydb as tinydb\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dython              0.6.1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# The Search for Categorical Correlation\n",
    "# https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "# https://github.com/shakedzy/dython\n",
    "# http://shakedzy.xyz/dython/\n",
    "if ! pip3 list | grep dython; then\n",
    "    pip3 install dython\n",
    "fi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinydb              4.2.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ! pip3 list | grep tinydb; then\n",
    "    pip3 install tinydb\n",
    "fi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "from pyspark import SparkContext, SparkFiles, SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "# https://github.com/shakedzy/dython\n",
    "from dython.nominal import associations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SQLContext(sc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "dataDir = '/Users/duane.hinkley/PycharmProjects/c772-capstone-project/juniper/.data/'\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.makedirs(dataDir)\n",
    "\n",
    "def init_df_raw():\n",
    "    dfRaw = import_by_url('https://github.com/dlhinkley/c772-capstone-project/raw/master/data/assessment_items.csv')\n",
    "\n",
    "    # Save to reuse\n",
    "    save_df(dfRaw, 'dfRaw')\n",
    "\n",
    "    # Only keep \"fully scored\" items\n",
    "    # Filter to learner_attempt_status = 'fully scored'\n",
    "    dfFlt = dfRaw.filter(F.col('learner_attempt_status') == 'fully scored')\n",
    "\n",
    "    # Change Date Fields from String to Timestamp Type\n",
    "    types = get_var_types()\n",
    "    for f in types['intervalVars']:\n",
    "      dfFlt = dfFlt.withColumn(f, F.col(f).cast(T.TimestampType() ) )\n",
    "\n",
    "    # Set default date values to null (years 2999 and 1900)\n",
    "    # Set empty dates to null\n",
    "    for f in types['intervalVars']:\n",
    "      # Change to empty if date is more than 30 months in past or future\n",
    "      dfFlt = dfFlt.withColumn(f, F.when( F.abs(F.months_between(F.col(f), F.current_timestamp() )) > 30, None ).otherwise( F.col(f) ) )\n",
    "\n",
    "    # Save to reuse\n",
    "    save_df(dfFlt, 'dfFlt')\n",
    "\n",
    "    return dfFlt\n",
    "\n",
    "\n",
    "def init_df_desc():\n",
    "    dfDesc = import_by_url('https://github.com/dlhinkley/c772-capstone-project/raw/master/data/descriptions.csv')\n",
    "    save_df(dfDesc, 'dfDesc')\n",
    "    return dfDesc\n",
    "\n",
    "\n",
    "def save_df(df, name):\n",
    "    df.repartition(1).write.mode('overwrite').parquet(dataDir + name + \".parquet\")\n",
    "\n",
    "def save_dict(data, name):\n",
    "    with open(dataDir + name + \".json\", \"w\") as f:\n",
    "      json.dump(data, f)\n",
    "\n",
    "def load_dict(name):\n",
    "    with open(dataDir + name + \".json\") as f:\n",
    "        out = json.load(f)\n",
    "\n",
    "    return out\n",
    "\n",
    "def load_df(name):\n",
    "    return spark.read.parquet(dataDir + name + \".parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_non_string_vars():\n",
    "    global shared\n",
    "    return group.identifierVars + group.continousVars + group.intervalVars + group.binaryVars\n",
    "\n",
    "def get_all_vars():\n",
    "        return group.nominalVars + get_non_string_vars();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tinydb import TinyDB, Query\n",
    "# Create Todo list\n",
    "def init_todo():\n",
    "  global td, dataDir\n",
    "  td = TinyDB(dataDir + 'todo.json')\n",
    "\n",
    "def add_todo(desc):\n",
    "  global td\n",
    "  q = Query()\n",
    "  if not td.contains(q.todo == desc):\n",
    "      td.insert({'todo': desc, 'finished': False})\n",
    "\n",
    "def list_todo():\n",
    "  global td\n",
    "  for item in td:\n",
    "     print(item)\n",
    "\n",
    "def finish_todo(desc):\n",
    "  global td\n",
    "  q = Query()\n",
    "  td.update({'finished': True}, q.todo == desc)\n",
    "\n",
    "def delete_todo(desc):\n",
    "  global td\n",
    "  q = Query()\n",
    "  td.remove(q.todo == desc)\n",
    "\n",
    "\n",
    "init_todo()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkFiles\n",
    "from datetime import datetime\n",
    "\n",
    "def import_by_url(url):\n",
    "  # Given a url to a csv file, import and return a dataframe\n",
    "  #\n",
    "  sc.addFile(url)\n",
    "  filename = os.path.basename(url)\n",
    "  file = \"file://\" + SparkFiles.get(filename)\n",
    "  return spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(file)\n",
    "\n",
    "\n",
    "def filter_default(dfIn, f1, f2):\n",
    "  # Given a dataframe and two date field names, returns the dataframe removing records\n",
    "  # where the f1 or f2 columns equal a default date\n",
    "  defaultDates = [\"2999-01-01 00:00:00\", \"1900-01-01 00:00:00\"]\n",
    "  return dfIn.filter( ~F.col(f1).isin(defaultDates) & ~F.col(f2).isin(defaultDates) )\n",
    "\n",
    "\n",
    "def date_stats(dfIn, f1, f2):\n",
    "  # Given a dataframe and two date field names, returns a new dataframe with the difference between\n",
    "  # the dates in minutes, hours and minutes\n",
    "  dfOut = filter_default(dfIn, f1, f2)\n",
    "\n",
    "  dfOut = dfOut.withColumn(\"minues\", (F.col(f1).cast(\"long\") - F.col(f2).cast(\"long\"))/60.).select(f1, f2, \"minues\")\n",
    "\n",
    "  dfOut = dfOut.withColumn(\"hours\", (F.col(f1).cast(\"long\") - F.col(f2).cast(\"long\"))/3600.).select(f1, f2, \"hours\", \"minues\")\n",
    "\n",
    "  return dfOut.withColumn(\"days\", (F.col(f1).cast(\"long\") - F.col(f2).cast(\"long\"))/86400.).select(\"days\", \"hours\", \"minues\")\n",
    "\n",
    "\n",
    "def annotate_plot(ax):\n",
    "  # Add total labels to plot\n",
    "  for p in ax.patches:\n",
    "      ax.annotate(\n",
    "        round(p.get_height(), 2),\n",
    "        (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        color='white',\n",
    "        fontweight='bold',\n",
    "        xytext=(0, -10),\n",
    "        textcoords='offset points')\n",
    "\n",
    "\n",
    "def date_boxplot(pdDf, title, ax = False):\n",
    "    # Given a dataframe of dates, create a boxplot of\n",
    "    # date distribution\n",
    "\n",
    "    if ax:\n",
    "        pdDf.boxplot(rot=270, figsize=[10,10], ax = ax)\n",
    "    else:\n",
    "        ax = pdDf.boxplot(rot=270, figsize=[10,10])\n",
    "\n",
    "    max = pdDf.max().max()\n",
    "    min = pdDf.min().min()\n",
    "    ytick = np.linspace(start=min, stop=max, num=12)\n",
    "    newLabels = [datetime.fromtimestamp(ts).strftime('%Y-%m-%d') for ts in ytick]\n",
    "    ax.set_yticks(ytick)\n",
    "    ax.set_yticklabels(labels=newLabels)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def single_val(df):\n",
    "    \"\"\" Give a dataframe, and a column to group by return a list of\n",
    "        single value variables\n",
    "    \"\"\"\n",
    "\n",
    "    inCols = []\n",
    "\n",
    "    for c in df.columns:\n",
    "        if df[c].unique().size == 1:\n",
    "           inCols.append(c)\n",
    "\n",
    "    return inCols\n",
    "\n",
    "def display_sv_cols(df, cols):\n",
    "    return df.select(cols).toPandas().head(1).transpose()\n",
    "\n",
    "def id_to_name(df, idVar, newVar, newIdList):\n",
    "  # Given a dataframe, id variable, new variable name and list of new ids\n",
    "  # add a new variable to the dataframe mapping the id to the array\n",
    "\n",
    "  # Save org ids to a list\n",
    "  oldIdList = [row[idVar] for row in df.select(idVar).distinct().orderBy(idVar).collect()]\n",
    "\n",
    "    # Create map\n",
    "  newIdMap = dict()\n",
    "  # Add letters to map\n",
    "  for i, val in enumerate(oldIdList):\n",
    "      newIdMap[val] = newIdList[i]\n",
    "\n",
    "  # Create mapping expression\n",
    "  mapping_expr = F.create_map([F.lit(x) for x in chain(*newIdMap.items())])\n",
    "\n",
    "  # Add org column with letter related to id\n",
    "  return df.withColumn(newVar, mapping_expr[df[idVar]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Return elements in whitelist\n",
    "def whitelist(l, whitelist):\n",
    "    if whitelist:\n",
    "        return l[ np.isIn(whitelist)]\n",
    "    else:\n",
    "        return l\n",
    "\n",
    "\n",
    "# Return a dictionary including arrays of variable names for each category\n",
    "# If dfColumns provied, return only values in dfColumns\n",
    "def get_var_cats(dfColumns = False):\n",
    "    cat = dict()\n",
    "    dfPd = load_df('dfDesc').toPandas()\n",
    "\n",
    "    cat['orgVars']               = whitelist( dfPd.loc[ dfPd['category'] == 'Organization' ].field.tolist(), dfColumns)\n",
    "    cat['sectionVars']           = whitelist( dfPd.loc[ dfPd['category'] == 'Section' ].field.tolist(), dfColumns)\n",
    "    cat['learnerVars']           = whitelist( dfPd.loc[ dfPd['category'] == 'Learner' ].field.tolist(), dfColumns)\n",
    "    cat['assessmentVars']        = whitelist( dfPd.loc[ dfPd['category'] == 'Assessment' ].field.tolist(), dfColumns)\n",
    "    cat['assignmentVars']        = whitelist( dfPd.loc[ dfPd['category'] == 'Assignment' ].field.tolist(), dfColumns)\n",
    "    cat['itemVars']              = whitelist( dfPd.loc[ dfPd['category'] == 'Item' ].field.tolist(), dfColumns)\n",
    "    cat['assignmentAttemptVars'] = whitelist( dfPd.loc[ dfPd['category'] == 'Assignment Attempt' ].field.tolist(), dfColumns)\n",
    "    cat['itemAttemptVars']       = whitelist( dfPd.loc[ dfPd['category'] == 'Item Attempt' ].field.tolist(), dfColumns)\n",
    "\n",
    "    return cat\n",
    "\n",
    "# Return a dictionary including arrays of variable names for each type\n",
    "# If dfColumns provied, return only values in dfColumns\n",
    "def get_var_types(dfColumns = False):\n",
    "    type = dict()\n",
    "    dfPd = load_df('dfDesc').toPandas()\n",
    "\n",
    "    type['identifierVars']  = whitelist( dfPd.loc[ dfPd['type'] == 'Categorical Identifier' ].field.tolist(), dfColumns)\n",
    "    type['nominalVars']     = whitelist( dfPd.loc[ dfPd['type'] == 'Categorical Nominal' ].field.tolist(), dfColumns)\n",
    "    type['continousVars']   = whitelist( dfPd.loc[ dfPd['type'] == 'Numeric Continuous' ].field.tolist(), dfColumns)\n",
    "    type['intervalVars']    = whitelist( dfPd.loc[ dfPd['type'] == 'Categorical Interval' ].field.tolist(), dfColumns)\n",
    "    type['binaryVars']      = whitelist( dfPd.loc[ dfPd['type'] == 'Categorical Binary' ].field.tolist(), dfColumns)\n",
    "\n",
    "    return type\n",
    "\n",
    "\n",
    "\n",
    "# Given a dataframe of datetime fields, return a matrix of the mean difference\n",
    "#\n",
    "def date_diff_map(df, title, scale = 'D'):\n",
    "\n",
    "    intVars = df.columns\n",
    "    intSize = len(intVars)\n",
    "    am      = pd.DataFrame(np.zeros(shape=(intSize , intSize)), columns = intVars, index = intVars)\n",
    "\n",
    "    for v1 in intVars:\n",
    "        for v2 in intVars:\n",
    "            if v1 != v2:\n",
    "\n",
    "                diff = ( df[v1] - df[v2] ) / np.timedelta64(1,scale)\n",
    "                am.at[v1,v2] = round( diff.mean())\n",
    "\n",
    "    ax = sn.heatmap(am, annot=True, fmt=\".0f\")\n",
    "    ax.set_title(title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}