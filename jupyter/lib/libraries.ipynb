{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Temp for local development\n",
    "import ssl\n",
    "import os\n",
    "# os.environ['PYSPARK_PYTHON'] = '/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6'\n",
    "import tinydb as tinydb\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "types_global = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dython              0.6.1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# The Search for Categorical Correlation\n",
    "# https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "# https://github.com/shakedzy/dython\n",
    "# http://shakedzy.xyz/dython/\n",
    "if ! pip3 list | grep dython; then\n",
    "    pip3 install dython\n",
    "fi\n",
    "\n",
    "if ! pip3 list | grep tinydb; then\n",
    "    pip3 install tinydb\n",
    "fi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "from pyspark import SparkContext, SparkFiles, SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "# https://github.com/shakedzy/dython\n",
    "from dython.nominal import associations\n",
    "\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "spark = SQLContext(sc)\n",
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "dataDir = '/Users/duane.hinkley/PycharmProjects/c772-capstone-project/jupyter/.data/'\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.makedirs(dataDir)\n",
    "\n",
    "def init_raw_df():\n",
    "    rawDf = import_by_url('https://github.com/dlhinkley/c772-capstone-project/raw/master/data/assessment_items.csv')\n",
    "\n",
    "    # Save to reuse\n",
    "    save_df(rawDf, 'rawDf')\n",
    "\n",
    "    return rawDf\n",
    "\n",
    "def filter_raw_df(rawDf):\n",
    "    # Only keep \"fully scored\" items\n",
    "    # Filter to learner_attempt_status = 'fully scored'\n",
    "    filterDf = rawDf.filter(\n",
    "            (F.col('assessment_item_response_id').isNull()  == False)\n",
    "          & (F.col('learner_attempt_status')  == 'fully scored')\n",
    "\n",
    "        )\n",
    "\n",
    "    # Change Date Fields from String to Timestamp Type\n",
    "    types = get_var_types()\n",
    "    for f in types['intervalVars']:\n",
    "      filterDf = filterDf.withColumn(f, F.col(f).cast(T.TimestampType() ) )\n",
    "\n",
    "    # Set default date values to null (years 2999 and 1900)\n",
    "    # Set empty dates to null\n",
    "    for f in types['intervalVars']:\n",
    "      # Change to empty if date is more than 30 months in past or future\n",
    "      filterDf = filterDf.withColumn(f, F.when( F.abs(F.months_between(F.col(f), F.current_timestamp() )) > 30, None ).otherwise( F.col(f) ) )\n",
    "\n",
    "    return filterDf\n",
    "\n",
    "\n",
    "def init_desc_df():\n",
    "    descDf = import_by_url('https://github.com/dlhinkley/c772-capstone-project/raw/master/data/descriptions.csv')\n",
    "    save_df(descDf, 'descDf')\n",
    "    return descDf\n",
    "\n",
    "\n",
    "def save_df(df, name):\n",
    "    df.repartition(1).write.mode('overwrite').parquet(dataDir + name + \".parquet\")\n",
    "\n",
    "def save_dict(data, name):\n",
    "    with open(dataDir + name + \".json\", \"w\") as f:\n",
    "      json.dump(data, f)\n",
    "\n",
    "def load_dict(name):\n",
    "    with open(dataDir + name + \".json\") as f:\n",
    "        out = json.load(f)\n",
    "\n",
    "    return out\n",
    "\n",
    "def load_df(name):\n",
    "    return spark.read.parquet(dataDir + name + \".parquet\")\n",
    "\n",
    "\n",
    "def get_non_string_vars():\n",
    "    global shared\n",
    "    return group.identifierVars + group.continuousVars + group.intervalVars + group.binaryVars\n",
    "\n",
    "def get_all_vars():\n",
    "        return group.nominalVars + get_non_string_vars();\n",
    "\n",
    "\n",
    "from tinydb import TinyDB, Query\n",
    "# Create Todo list\n",
    "def init_todo():\n",
    "  global td, dataDir\n",
    "  td = TinyDB(dataDir + 'todo.json')\n",
    "\n",
    "def add_todo(desc):\n",
    "  global td\n",
    "  q = Query()\n",
    "  if not td.contains(q.todo == desc):\n",
    "      td.insert({'todo': desc, 'finished': False})\n",
    "\n",
    "  print(\"Todo: \" + desc)\n",
    "\n",
    "\n",
    "def list_todo(finished = None):\n",
    "  global td\n",
    "\n",
    "  for item in td:\n",
    "    if (finished != None):\n",
    "        if (item['finished'] == finished):\n",
    "           print(item)\n",
    "    else:\n",
    "        print(item)\n",
    "\n",
    "\n",
    "def finish_todo(desc):\n",
    "  global td\n",
    "  q = Query()\n",
    "  td.update({'finished': True}, q.todo == desc)\n",
    "  print(\"Finished: \" + desc)\n",
    "\n",
    "\n",
    "def delete_todo(desc):\n",
    "  global td\n",
    "  q = Query()\n",
    "  td.remove(q.todo == desc)\n",
    "\n",
    "\n",
    "init_todo()\n",
    "\n",
    "\n",
    "import os\n",
    "from pyspark import SparkFiles\n",
    "from datetime import datetime\n",
    "\n",
    "def import_by_url(url):\n",
    "  # Given a url to a csv file, import and return a dataframe\n",
    "  #\n",
    "  sc.addFile(url)\n",
    "  filename = os.path.basename(url)\n",
    "  file = \"file://\" + SparkFiles.get(filename)\n",
    "  return spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(file)\n",
    "\n",
    "\n",
    "def filter_default(dfIn, f1, f2):\n",
    "  # Given a dataframe and two date field names, returns the dataframe removing records\n",
    "  # where the f1 or f2 columns equal a default date\n",
    "  defaultDates = [\"2999-01-01 00:00:00\", \"1900-01-01 00:00:00\"]\n",
    "  return dfIn.filter( ~F.col(f1).isin(defaultDates) & ~F.col(f2).isin(defaultDates) )\n",
    "\n",
    "\n",
    "def date_stats(dfIn, f1, f2):\n",
    "  # Given a dataframe and two date field names, returns a new dataframe with the difference between\n",
    "  # the dates in minutes, hours and minutes\n",
    "  dfOut = filter_default(dfIn, f1, f2)\n",
    "\n",
    "  dfOut = dfOut.withColumn(\"minues\", (F.col(f1).cast(\"long\") - F.col(f2).cast(\"long\"))/60.).select(f1, f2, \"minues\")\n",
    "\n",
    "  dfOut = dfOut.withColumn(\"hours\", (F.col(f1).cast(\"long\") - F.col(f2).cast(\"long\"))/3600.).select(f1, f2, \"hours\", \"minues\")\n",
    "\n",
    "  return dfOut.withColumn(\"days\", (F.col(f1).cast(\"long\") - F.col(f2).cast(\"long\"))/86400.).select(\"days\", \"hours\", \"minues\")\n",
    "\n",
    "\n",
    "def annotate_plot(ax):\n",
    "  # Add total labels to plot\n",
    "  for p in ax.patches:\n",
    "      ax.annotate(\n",
    "        round(p.get_height(), 2),\n",
    "        (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        color='white',\n",
    "        fontweight='bold',\n",
    "        xytext=(0, -10),\n",
    "        textcoords='offset points')\n",
    "\n",
    "\n",
    "def date_boxplot(df, title, ax = False):\n",
    "    # Given a dataframe of datestimes, create a boxplot of\n",
    "    # date distribution\n",
    "    types = get_var_types()\n",
    "    # Convert to timestamps\n",
    "    pdDf = df.select(* (F.unix_timestamp(c).alias(c) for c in types['intervalVars'] if c in df.columns) ).toPandas()\n",
    "\n",
    "    if ax:\n",
    "        pdDf.boxplot(rot=270, figsize=[10,10], ax = ax)\n",
    "    else:\n",
    "        ax = pdDf.boxplot(rot=270, figsize=[10,10])\n",
    "\n",
    "    # Min and Max date plus and minus one month\n",
    "    max = pd.to_datetime(pdDf.max().max(), unit='s') + pd.DateOffset(months=1)\n",
    "    min = pd.to_datetime(pdDf.min().min(), unit='s') - pd.DateOffset(months=1)\n",
    "\n",
    "    # Date labels by month\n",
    "    yLabels = pd.date_range(start=min.date(), end=max.date(), freq='MS')\n",
    "    # Convert ticks to unix timestamp (int)\n",
    "    ytick = [t.value // 10 ** 9 for t in yLabels]\n",
    "    # Year and month readable labels\n",
    "    newLabels = [ts.strftime('%Y-%m') for ts in yLabels]\n",
    "    ax.set_yticks(ytick)\n",
    "    ax.set_yticklabels(labels = newLabels)\n",
    "    # Add category to labels\n",
    "    labels = [types[ label.get_text() ] for label in ax.get_xticklabels()]\n",
    "    ax.set_xticklabels(labels = labels)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def type_cat(var):\n",
    "    # return types_global[var]\n",
    "    return var\n",
    "\n",
    "# Create a udf for pyspark\n",
    "type_cat_udf = F.udf(type_cat)\n",
    "\n",
    "\n",
    "def distinct_val(df, skipKnown = True):\n",
    "\n",
    "    # Given a dataframe, return the distinct values\n",
    "    knowCols = ['assignment_late_submission',\n",
    "                'learner_attempt_status',\n",
    "                'is_deleted']\n",
    "    if (skipKnown):\n",
    "        cols = blacklist(df.columns, knowCols)\n",
    "    else:\n",
    "        cols = df.columns\n",
    "\n",
    "    # Get count of values in each column\n",
    "    svDf = df.agg(*(F.countDistinct( F.when( F.col(c).isNull(), 'null').otherwise( F.col(c).cast('string') ) ).alias( c ) for c in cols))\n",
    "\n",
    "    # Save dataframe to list\n",
    "    sv = svDf.collect()[0]\n",
    "    # Get columns with a count of one\n",
    "    svCols = [c for c in cols if sv[c] == 1]\n",
    "    svCols.sort()\n",
    "    # Return one row of panda dataframe with count of 1\n",
    "    pdDf =  df.select( svCols ).limit(1).toPandas()\n",
    "    # Change the variable names to variables with labels\n",
    "    return col_to_label( pdDf ).transpose()\n",
    "\n",
    "\n",
    "def id_to_name(df, idVar, newVar, newIdList):\n",
    "  # Given a dataframe, id variable, new variable name and list of new ids\n",
    "  # add a new variable to the dataframe mapping the id to the array\n",
    "\n",
    "  # Save org ids to a list\n",
    "  oldIdList = [row[idVar] for row in df.select(idVar).distinct().orderBy(idVar).collect()]\n",
    "\n",
    "    # Create map\n",
    "  newIdMap = dict()\n",
    "  # Add letters to map\n",
    "  for i, val in enumerate(oldIdList):\n",
    "      newIdMap[val] = newIdList[i]\n",
    "\n",
    "  # Create mapping expression\n",
    "  mapping_expr = F.create_map([F.lit(x) for x in chain(*newIdMap.items())])\n",
    "\n",
    "  # Add org column with letter related to id\n",
    "  return df.withColumn(newVar, mapping_expr[df[idVar]])\n",
    "\n",
    "\n",
    "\n",
    "# Return elements in whitelist\n",
    "def whitelist(l, whitelist):\n",
    "    if whitelist:\n",
    "        return [x for x in l if x in whitelist]\n",
    "    else:\n",
    "        return l\n",
    "\n",
    "# Return elements except those in blacklist\n",
    "def blacklist(l, blacklist):\n",
    "    if blacklist:\n",
    "        return [x for x in l if x not in blacklist]\n",
    "    else:\n",
    "        return l\n",
    "\n",
    "\n",
    "# Return a dictionary including arrays of variable names for each category\n",
    "# If dfColumns provied, return only values in dfColumns\n",
    "def get_var_cats(dfColumns = False):\n",
    "    cat = dict()\n",
    "    dfPd = load_df('descDf').toPandas()\n",
    "\n",
    "    cat['orgVars']               = whitelist( dfPd.loc[ dfPd['category'] == 'Organization' ].field.tolist(), dfColumns)\n",
    "    cat['sectionVars']           = whitelist( dfPd.loc[ dfPd['category'] == 'Section' ].field.tolist(), dfColumns)\n",
    "    cat['learnerVars']           = whitelist( dfPd.loc[ dfPd['category'] == 'Learner' ].field.tolist(), dfColumns)\n",
    "    cat['assessmentVars']        = whitelist( dfPd.loc[ dfPd['category'] == 'Assessment' ].field.tolist(), dfColumns)\n",
    "    cat['assignmentVars']        = whitelist( dfPd.loc[ dfPd['category'] == 'Assignment' ].field.tolist(), dfColumns)\n",
    "    cat['itemVars']              = whitelist( dfPd.loc[ dfPd['category'] == 'Item' ].field.tolist(), dfColumns)\n",
    "    cat['assignmentAttemptVars'] = whitelist( dfPd.loc[ dfPd['category'] == 'Assignment Attempt' ].field.tolist(), dfColumns)\n",
    "    cat['itemAttemptVars']       = whitelist( dfPd.loc[ dfPd['category'] == 'Item Attempt' ].field.tolist(), dfColumns)\n",
    "\n",
    "    return cat\n",
    "\n",
    "# Return a dictionary including arrays of variable names for each type\n",
    "# If dfColumns provied, return only values in dfColumns\n",
    "def get_var_types(dfColumns = False):\n",
    "\n",
    "    if (types_global == None):\n",
    "\n",
    "        type = dict()\n",
    "        descDf = load_df('descDf')\n",
    "\n",
    "        type['identifierVars']  = whitelist( variable_types_label(descDf, 'Categorical Identifier').variable.tolist(), dfColumns)\n",
    "        type['nominalVars']     = whitelist( variable_types_label(descDf, 'Categorical Nominal').variable.tolist(),  dfColumns)\n",
    "        type['continuousVars']  = whitelist( variable_types_label(descDf, 'Numeric Continuous').variable.tolist(),   dfColumns)\n",
    "        type['intervalVars']    = whitelist( variable_types_label(descDf, 'Categorical Interval').variable.tolist(), dfColumns)\n",
    "        type['binaryVars']      = whitelist( variable_types_label(descDf, 'Categorical Binary').variable.tolist(),   dfColumns)\n",
    "        type['durationVars']    = [\n",
    "                                    'item_attempt_duration_mins',\n",
    "                                    'student_duration_mins',\n",
    "                                    'timeliness_duration_mins'\n",
    "                                    ]\n",
    "\n",
    "        type['identifierVarsLabels']  = whitelist( variable_types_label(descDf, 'Categorical Identifier').variable_label.tolist(), dfColumns)\n",
    "        type['nominalVarsLabels']     = whitelist( variable_types_label(descDf, 'Categorical Nominal').variable_label.tolist(),  dfColumns)\n",
    "        type['continuousVarsLabels']  = whitelist( variable_types_label(descDf, 'Numeric Continuous').variable_label.tolist(),   dfColumns)\n",
    "        type['intervalVarsLabels']    = whitelist( variable_types_label(descDf, 'Categorical Interval').variable_label.tolist(), dfColumns)\n",
    "        type['binaryVarsLabels']      = whitelist( variable_types_label(descDf, 'Categorical Binary').variable_label.tolist(),   dfColumns)\n",
    "\n",
    "\n",
    "        # Sort\n",
    "        for key in type:\n",
    "            type[key].sort()\n",
    "\n",
    "        # Add variable to label map\n",
    "        for cat in ['identifierVars', 'nominalVars', 'continuousVars', 'intervalVars', 'binaryVars']:\n",
    "            for p in range( len( type[cat] ) ):\n",
    "                type[ type[cat][p] ] = type[ cat + 'Labels'][p]\n",
    "\n",
    "        return type\n",
    "    else:\n",
    "        return types_global\n",
    "\n",
    "\n",
    "# Given a dataframe of datetime fields, return a matrix of the mean difference\n",
    "#\n",
    "def date_diff_map(df, title, scale = 'D', ax = None):\n",
    "\n",
    "    intVars = df.columns\n",
    "    intSize = len(intVars)\n",
    "    am      = pd.DataFrame(np.zeros(shape=(intSize , intSize)), columns = intVars, index = intVars)\n",
    "\n",
    "\n",
    "    for v1 in intVars:\n",
    "        for v2 in intVars:\n",
    "            if v1 != v2:\n",
    "\n",
    "                mean = ( ( df[v1] - df[v2] ) / np.timedelta64(1,scale) ).mean()\n",
    "                am.at[v1,v2] = mean\n",
    "\n",
    "    ax = sn.heatmap(am, annot=True, fmt=\".0f\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def get_random_sample(df):\n",
    "    return df.sample(False, .10, 8764664)\n",
    "\n",
    "def mean_hours_assignment_interval(df, ax=None):\n",
    "    assignIntVars = [\n",
    "        'student_start_datetime',\n",
    "        'was_in_progress_datetime',\n",
    "        'scored_datetime',\n",
    "        'was_submitted_datetime_actual',\n",
    "        'student_stop_datetime',\n",
    "        'was_fully_scored_datetime',\n",
    "    ]\n",
    "    date_diff_map(df.select(assignIntVars).toPandas(), \"Mean Hours Between Assignment Interval Vars\", 'h', ax)\n",
    "\n",
    "\n",
    "def dual_mean_hours_assignment(df1, df2, title1='', title2='', main=''):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, sharex=True)\n",
    "\n",
    "    mean_hours_assignment_interval(df1, ax1)\n",
    "    ax1.set_title(title1)\n",
    "\n",
    "    mean_hours_assignment_interval(df2, ax2)\n",
    "    ax2.set_title(title2)\n",
    "\n",
    "    plt.suptitle(main)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def remove_null_student_dates(df):\n",
    "\n",
    "    return df.filter(\n",
    "          (F.col('student_start_datetime').isNull() == False)\n",
    "        | (F.col('student_stop_datetime').isNull() == False)\n",
    "    )\n",
    "\n",
    "\n",
    "def impute_3422_null_dates(filterDf):\n",
    "\n",
    "    # Get sample to extract means\n",
    "    pdDf = get_random_sample(filterDf).select(\n",
    "        'student_stop_datetime',\n",
    "        'scored_datetime').toPandas()\n",
    "\n",
    "    # Calculate mean difference in seconds\n",
    "    mScoredDate = ( (pdDf['scored_datetime'] - pdDf['student_stop_datetime'])     / np.timedelta64(1, 's') ).mean()\n",
    "\n",
    "\n",
    "    return filterDf.withColumn(\n",
    "                \"scored_datetime_imputed\",\n",
    "                F.col(\"scored_datetime\").isNull()\n",
    "            ).withColumn(\n",
    "                \"scored_datetime\",\n",
    "                F.when(\n",
    "                    F.col(\"scored_datetime\").isNull(),\n",
    "                    (F.unix_timestamp(\"student_stop_datetime\") - mScoredDate).cast('timestamp')\n",
    "                ).otherwise( F.col(\"scored_datetime\") )\n",
    "    )\n",
    "\n",
    "\n",
    "def impute_9965_null_dates(df):\n",
    "\n",
    "    # Get sample to extract means\n",
    "    pdDf = get_random_sample(df).select('student_start_datetime', 'student_stop_datetime', 'was_in_progress_datetime', 'scored_datetime').toPandas()\n",
    "\n",
    "    # Calculate mean difference in seconds\n",
    "    meanDiff = ( (pdDf['was_in_progress_datetime'] - pdDf['student_start_datetime'])     / np.timedelta64(1, 's') ).mean()\n",
    "\n",
    "\n",
    "    return df.withColumn(\n",
    "                \"scored_datetime\",\n",
    "                F.when(\n",
    "                    F.col(\"was_in_progress_datetime\").isNull(),\n",
    "                    (F.unix_timestamp(\"student_start_datetime\") - meanDiff).cast('timestamp')\n",
    "                ).otherwise( F.col(\"was_in_progress_datetime\") )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Given a dataframe and variable name, return the value names and counts for that variable\n",
    "def count_values(df, f):\n",
    "   return df.groupBy(f).count().orderBy('count', ascending=False)\n",
    "\n",
    "\n",
    "def response_correctness_bar_plot(df, varName, ax=None):\n",
    "    pdDf = df.groupBy(varName).count().orderBy('count', ascending=False).toPandas()\n",
    "    ax = pdDf.plot(kind='bar', ax=ax)\n",
    "    labels = pdDf[varName].fillna('null').tolist()\n",
    "    ax.set_xticklabels(labels=labels)\n",
    "    ax.set_title('Values ' + varName)\n",
    "    annotate_plot(ax)\n",
    "\n",
    "\n",
    "# Create a binary catagorical variable for final_score_unweighted\n",
    "def add_zero_final_score_var(df):\n",
    "    return df.withColumn(\n",
    "                \"zero_score\",\n",
    "                  F.when( F.col('final_score_unweighted') == 0, 'Yes').otherwise(\"No\")\n",
    "                )\n",
    "\n",
    "def add_zero_raw_score_var(df):\n",
    "    return df.withColumn(\n",
    "                \"zero_score\",\n",
    "                  F.when( F.col('raw_score').isNull(), 'Null')\n",
    "                      .otherwise( F.when( F.col('raw_score') == 0, 'Yes').otherwise(\"No\")   )\n",
    "                )\n",
    "\n",
    "\n",
    "# Create a bar chart counting countvar for each groupvar\n",
    "def num_group_bar_chart(df, groupByVar, countVar, countAlias, title, ax = None):\n",
    "  sByO = df.groupBy(groupByVar).agg(F.countDistinct(countVar).alias(countAlias)).orderBy(groupByVar)\n",
    "  pdDf = sByO.toPandas()\n",
    "\n",
    "  # Add mean\n",
    "  mean = sByO.agg(F.round(F.avg(F.col(countAlias))).alias('mean')).collect()[0][0]\n",
    "  # Append row with mean\n",
    "  pdDf = pdDf.append({groupByVar: 'mean', countAlias: mean}, ignore_index=True)\n",
    "\n",
    "  axa = pdDf.plot(groupByVar,countAlias, kind='bar', ax=ax, title=title)\n",
    "  annotate_plot(axa)\n",
    "  if (ax == None):\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def num_sections_by_org_bar_chart(df, ax = None, title='Num Sections by Organization'):\n",
    "    num_group_bar_chart(df, 'org_id', 'section_id', 'sections', title, ax)\n",
    "\n",
    "\n",
    "def num_learners_by_org_bar_chart(df):\n",
    "    num_group_bar_chart(df, 'org_id', 'learner_id', 'learners', 'Num Learners by Organization')\n",
    "\n",
    "\n",
    "def mean_group_bar_chart(df, group1, group2, countVar, countAlias, title='', ax=None):\n",
    "  lByS = df.groupBy(group1, group2).agg(F.countDistinct(countVar).alias(countAlias))\n",
    "  # Av\n",
    "  lBySMean = lByS.groupBy(group1).agg( F.avg(countAlias).alias(countAlias) ).orderBy(group1)\n",
    "  pdDf = lBySMean.toPandas()\n",
    "\n",
    "  # Add mean\n",
    "  mean = lBySMean.agg(F.round(F.avg(F.col(countAlias))).alias('mean')).collect()[0][0]\n",
    "  # Append row with mean\n",
    "  pdDf = pdDf.append({group1: 'mean', countAlias: mean}, ignore_index=True)\n",
    "\n",
    "  axo = pdDf.plot.bar(group1,countAlias, ax=ax, title=title)\n",
    "  annotate_plot(axo)\n",
    "\n",
    "  if (ax == None):\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def mean_sec_learners_by_org_bar_chart(df):\n",
    "    mean_group_bar_chart(df, 'org_id','section_id', 'learner_id', 'learners', 'Mean Section Learners by Organization')\n",
    "\n",
    "\n",
    "def mean_sec_assess_by_org_bar_chart(df, ax=None, title='Mean Section Assessments by Organization'):\n",
    "    mean_group_bar_chart(df, 'org_id','section_id', 'assessment_id', 'assessments', ax=ax, title=title)\n",
    "\n",
    "\n",
    "def mean_assess_by_org_bar_chart(df, ax=None, title = 'Mean Learners Assessments by Organization'):\n",
    "    mean_group_bar_chart(df, 'org_id','learner_id', 'assessment_id', 'assessments',ax=ax, title=title,)\n",
    "\n",
    "# Mean Scores by Organization\n",
    "def mean_scores_by_orgs_bar_chart(df):\n",
    "  sByO = df.groupBy('org_id').agg( F.avg('final_score_unweighted').alias('scores') ).orderBy('org_id')\n",
    "\n",
    "  pdDf = sByO.toPandas()\n",
    "\n",
    "  # Add mean\n",
    "  meanAssess = sByO.agg(F.round(F.avg(F.col('scores'))).alias('mean')).collect()[0][0]\n",
    "  # Append row with mean\n",
    "  pdDf = pdDf.append({'org_id': 'mean', 'scores': meanAssess}, ignore_index=True)\n",
    "\n",
    "  ax = pdDf.plot.bar('org_id','scores', title='Mean Scores by Organization')\n",
    "  annotate_plot(ax)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def crosstab_percent(table):\n",
    "    return table.apply(lambda r: round(r/r.sum() * 100), axis=1)\n",
    "\n",
    "\n",
    "# Given two dataframes of datetime fields, with optional titles, return two side by side boxplots\n",
    "#\n",
    "def dual_date_boxplot(df1, df2, title1='', title2='', main=''):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, sharex=True)\n",
    "\n",
    "    date_boxplot( df1, title1, ax1)\n",
    "    date_boxplot(df2, title2, ax2)\n",
    "\n",
    "    plt.suptitle(main)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Given two pandas dataframes with one variable each, return two histograms\n",
    "def dual_hist(pdDf1, pdDf2, title1='', title2='', main=''):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\n",
    "\n",
    "    pdDf1.hist(ax=ax1)\n",
    "    ax1.set_title(title1)\n",
    "\n",
    "    pdDf2.hist(ax=ax2)\n",
    "    ax2.set_title(title2)\n",
    "\n",
    "    plt.suptitle(main)\n",
    "    plt.show()\n",
    "\n",
    "# Given two dataframes, create two heatmaps of all variables in dataframe\n",
    "def dual_assoc_heatmap(df1, df2, title1 = '', title2 = '', main = '', figsize=(10,5)):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, sharex=True, figsize=figsize)\n",
    "\n",
    "    associations( df1.toPandas(), nan_replace_value='null', ax=ax1, plot=False )\n",
    "    ax1.set_title(title1)\n",
    "\n",
    "    associations( df2.toPandas(), nan_replace_value='null', ax=ax2, plot=False )\n",
    "    ax2.set_title(title2)\n",
    "\n",
    "    plt.suptitle(main)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Impute the 4446 null dates with the mean difference of was_submitted_datetime_actual and student_stop_datetime\n",
    "def impute_4446_null_dates(df):\n",
    "\n",
    "    # Get sample to extract means\n",
    "    pdDf = df.select( 'scored_datetime', 'was_submitted_datetime_actual').toPandas()\n",
    "\n",
    "    # Calculate mean difference in seconds\n",
    "    meanDiff = ( (pdDf['scored_datetime'] - pdDf['was_submitted_datetime_actual'])     / np.timedelta64(1, 's') ).mean()\n",
    "\n",
    "\n",
    "    return df.withColumn( # Imputed flag\n",
    "                    \"was_submitted_datetime_actual_imputed\",\n",
    "                    F.col('was_submitted_datetime_actual').isNull()\n",
    "            ).withColumn(\n",
    "                \"was_submitted_datetime_actual\",\n",
    "                F.when(\n",
    "                        (F.col('scored_datetime_imputed') == False)  # don't impute with imputed value\n",
    "                      & (F.col('was_submitted_datetime_actual').isNull())\n",
    "                      & (F.col('final_score_unweighted') > 0),\n",
    "                    (F.unix_timestamp(\"scored_datetime\") - meanDiff).cast('timestamp')\n",
    "                ).otherwise( F.col(\"was_submitted_datetime_actual\") )\n",
    "            )\n",
    "\n",
    "\n",
    "def impute_number_of_learners(cleanDf):\n",
    "\n",
    "    # Calculate number of learners on Filtered\n",
    "    dfCount = cleanDf.groupBy('assessment_instance_id', 'number_of_learners').agg(\n",
    "        F.countDistinct('learner_id').alias('number_of_learners_calc')\n",
    "    ).select('assessment_instance_id', 'number_of_learners_calc')\n",
    "\n",
    "    # Update with calculated value\n",
    "    cleanDf = cleanDf.join(dfCount, on=['assessment_instance_id'], how='left')\n",
    "\n",
    "    # Drop incorrect number of learners\n",
    "    cleanDf = cleanDf.drop('number_of_learners')\n",
    "\n",
    "    # Rename calculated to original\n",
    "    return cleanDf.withColumnRenamed(\"number_of_learners_calc\",\"number_of_learners\")\n",
    "\n",
    "\n",
    "# Reduce the number of levels in item_type_code_name\n",
    "def reduce_type_code_levels(cleanDf):\n",
    "    # Combine fillInTheBlank and FillinBlankResponse\n",
    "    cleanDf = cleanDf.withColumn(\"item_type_code_name\", F.when( F.col(\"item_type_code_name\") == \"FillinBlankResponse\", \"fillInTheBlank\" ).otherwise(F.col(\"item_type_code_name\")) )\n",
    "\n",
    "    # Combine multipleChoice and MultipleChoiceResponse\n",
    "    cleanDf = cleanDf.withColumn(\"item_type_code_name\", F.when( F.col(\"item_type_code_name\") == \"MultipleChoiceResponse\", \"multipleChoice\" ).otherwise(F.col(\"item_type_code_name\")) )\n",
    "\n",
    "    # Total count\n",
    "    tot = cleanDf.filter(F.col(\"item_type_code_name\").isNull() == False).count()\n",
    "\n",
    "    freqTable = cleanDf.groupBy(\"item_type_code_name\") \\\n",
    "                   .count() \\\n",
    "                   .withColumnRenamed('count', 'cnt_per_group') \\\n",
    "                   .withColumn('perc_of_count_total', ( F.col('cnt_per_group') / tot) * 100 ) \\\n",
    "                   .orderBy(\"cnt_per_group\", ascending=False)\n",
    "\n",
    "    # freqTable.show(50, False)\n",
    "\n",
    "    # We only want five levels, so convert everything below 6% to other\n",
    "\n",
    "\n",
    "    otherRows    = freqTable.filter(\"perc_of_count_total < 6\")\n",
    "    otherLevels  = [row['item_type_code_name'] for row in otherRows.select(\"item_type_code_name\").collect()]\n",
    "\n",
    "    return cleanDf.withColumn(\"item_type_code_name\", F.when( F.col(\"item_type_code_name\").isin(otherLevels) | F.col(\"item_type_code_name\").isNull() , \"Other\" ).otherwise(F.col(\"item_type_code_name\")) )\n",
    "\n",
    "\n",
    "# Return dataframe of min, max, mean grouped by column\n",
    "def group_by_describe(df, groupBy, statsCol):\n",
    "    return df.groupBy(groupBy).agg(\n",
    "          F.round( F.count(statsCol) ).alias('count'),\n",
    "          F.round( F.min(statsCol) ).alias('min'),\n",
    "          F.round( F.avg(statsCol) ).alias('mean'),\n",
    "          F.round( F.max(statsCol) ).alias('max')\n",
    "    )\n",
    "\n",
    "\n",
    "# Given a dataframe and list of cols, displays the min, max, null, and unique value counts\n",
    "def date_min_max_null_unique(df, cols):\n",
    "    for f in cols:\n",
    "      print (f)\n",
    "      df.agg(\n",
    "        F.countDistinct(f).alias('unique'),\n",
    "        F.count(F.when(F.col(f).isNull(), f)).alias('null'),\n",
    "        F.min(f).alias('min'),\n",
    "        F.max(f).alias('max')\n",
    "     ).show(1, False)\n",
    "\n",
    "# Given a dataframe of dates and columns, return a pandas datafram of statistics\n",
    "def date_statisticts(df, cols):\n",
    "    cols.sort()\n",
    "    distinct = df.agg(\n",
    "        *(F.countDistinct(F.col(c)).alias(c) for c in cols)\n",
    "    ).collect()[0]\n",
    "    null = df.agg(\n",
    "        *(F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in cols)\n",
    "    ).collect()[0]\n",
    "    min = df.agg(\n",
    "        *(F.min(F.col(c).cast(T.DateType())).alias(c) for c in cols)\n",
    "    ).collect()[0]\n",
    "    max = df.agg(\n",
    "        *(F.max(F.col(c).cast(T.DateType())).alias(c) for c in cols)\n",
    "    ).collect()[0]\n",
    "    plotdata = pd.DataFrame({\n",
    "        \"distinct\": distinct,\n",
    "        \"null\":null,\n",
    "        \"min\":min,\n",
    "        \"max\":max,\n",
    "    },\n",
    "    index=cols)\n",
    "\n",
    "    return plotdata\n",
    "\n",
    "\n",
    "def null_zero_counts(df, cols):\n",
    "    cols.sort()\n",
    "    null = df.agg(\n",
    "        *(F.count(F.when(F.col(c).isNull(), c)).alias('null') for c in cols)\n",
    "    ).collect()[0]\n",
    "    zero = df.agg(\n",
    "        *(F.count(F.when(F.col(c) == 0, c)).alias(\"zero\") for c in cols)\n",
    "    ).collect()[0]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"null\": null,\n",
    "        \"zero\": zero,\n",
    "    }, index=cols)\n",
    "\n",
    "\n",
    "def unique_nulls(df, cols):\n",
    "    cols.sort()\n",
    "    null = df.agg(\n",
    "        *(F.count(F.when(F.col(c).isNull(), c)).alias('null') for c in cols)\n",
    "    ).collect()[0]\n",
    "    unique = df.agg(\n",
    "        *(F.countDistinct(c).alias('unique') for c in cols)\n",
    "    ).collect()[0]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"null\": null,\n",
    "        \"unique\": unique,\n",
    "    }, index=cols)\n",
    "\n",
    "# Given a full name of multiple words, return the initials in lower case\n",
    "def initials(fullname):\n",
    "  xs = (fullname)\n",
    "  name_list = xs.split()\n",
    "\n",
    "  initials = \"\"\n",
    "\n",
    "  for name in name_list:  # go through each name\n",
    "    initials += name[0].lower()  # append the initial\n",
    "\n",
    "  return initials\n",
    "\n",
    "# Given a full name of multiple words,\n",
    "# return the initials in lower case wrapped in parenthesis\n",
    "def wrap_initials(fullname):\n",
    "    return '(' + initials(fullname) + ')'\n",
    "\n",
    "# Create a udf for pyspark\n",
    "initials_udf = F.udf(wrap_initials)\n",
    "\n",
    "#Given a dataframe and variable type, return the names of variables\n",
    "def variable_types(df, type):\n",
    "    return df.filter(F.col('type') == type ).select(\n",
    "            F.concat_ws(' ', F.col('category'),initials_udf( F.col('category') ) ).alias('category'),\n",
    "            F.col('field').alias('variable')\n",
    "        ).orderBy('category', 'variable').toPandas()\n",
    "\n",
    "\n",
    "#Given a dataframe and variable type, return the names of\n",
    "# variables with cat label as pandas dataframe\n",
    "def variable_types_label(df, type = None):\n",
    "    return df.filter(F.col('type') == type ).select(\n",
    "            F.concat_ws(' ', F.col('field'),initials_udf( F.col('category') ) ).alias('variable_label'),\n",
    "            F.col('field').alias('variable')\n",
    "        ).orderBy('category', 'variable').toPandas()\n",
    "\n",
    "\n",
    "# Given a pandas dataframe remap the column name to columns with labels\n",
    "def col_to_label(pdDf):\n",
    "    typeDict = get_var_types()\n",
    "    pdDf.columns = pdDf.columns.to_series().map(typeDict)\n",
    "    return pdDf\n",
    "\n",
    "# Removes start dates after stop dates and null dates\n",
    "def clean_item_attempt_dates(df):\n",
    "   return df.filter(\n",
    "       F.col('item_attempt_start_datetime_utc').cast('long') <= F.col('item_attempt_end_datetime_utc').cast('long')\n",
    "   )\n",
    "\n",
    "# Adds the duration between start and stop of the attempt and each item\n",
    "def add_attempt_duration(df):\n",
    "    return df.withColumn(\n",
    "        'item_attempt_duration_mins', (F.col('item_attempt_end_datetime_utc').cast('long') - F.col('item_attempt_start_datetime_utc').cast('long')) / 60\n",
    "    ).withColumn(\n",
    "        'student_duration_mins', (F.col('student_stop_datetime').cast('long') - F.col('student_start_datetime').cast('long')) / 60\n",
    "    ).withColumn(\n",
    "        'timeliness_duration_mins', (F.col('assignment_due_date').cast('long') - F.col('student_start_datetime').cast('long')) / 60\n",
    "    )\n",
    "\n",
    "def remove_attempt_stop_dates_before_start_dates(df):\n",
    "    inList = df.filter(\n",
    "         F.col('item_attempt_end_datetime_utc') < F.col('item_attempt_start_datetime_utc')\n",
    "    ).select('assessment_item_response_id').toPandas().assessment_item_response_id.tolist()\n",
    "\n",
    "    return df.filter( F.col('assessment_item_response_id').isin(inList) == False )\n",
    "\n",
    "\n",
    "def rotate_matrix_labels(pdDf, axs):\n",
    "    n = len(pdDf.columns)\n",
    "    for x in range(n):\n",
    "        for y in range(n):\n",
    "            # to get the axis of subplots\n",
    "            ax = axs[x, y]\n",
    "            # to make x axis name vertical\n",
    "            ax.xaxis.label.set_rotation(90)\n",
    "            # to make y axis name horizontal\n",
    "            ax.yaxis.label.set_rotation(0)\n",
    "            # to make sure y axis names are outside the plot area\n",
    "            ax.yaxis.labelpad = 50\n",
    "\n",
    "\n",
    "def logrithmic_histogram(pdDf, tickStep = 5000):\n",
    "    import matplotlib.ticker as plticker\n",
    "\n",
    "    ax = pdDf.plot(kind='hist', logy=True, bins=100, bottom=0.1, rot=270)\n",
    "\n",
    "    # Add more steps to x labels\n",
    "    start, end = ax.get_xlim()\n",
    "    ax.xaxis.set_ticks(np.arange(start, end, tickStep))\n",
    "\n",
    "    loc = plticker.MultipleLocator(tickStep) # this locator puts ticks at regular intervals\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "\n",
    "    # Remove scientific notation\n",
    "    for axis in [ax.xaxis, ax.yaxis]:\n",
    "        axis.set_major_formatter(ScalarFormatter())\n",
    "\n",
    "def get_iqr_filter(df, col):\n",
    "    # Only positive (negative are a different problem)\n",
    "    pdDf = df.filter(F.col(col) >= 0).select(col).toPandas()\n",
    "    Q1 = pdDf[col].quantile(0.25)\n",
    "    Q3 = pdDf[col].quantile(0.75)\n",
    "    median = pdDf[col].quantile(0.50)\n",
    "\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lowFilter = (Q1 - 1.5 * IQR)\n",
    "    highFilter = (Q3 + 1.5 * IQR)\n",
    "\n",
    "    return lowFilter, highFilter, median\n",
    "\n",
    "# Given a dataframe and column, impute outliers over IQR with mean\n",
    "def iqr_impute(df, col):\n",
    "\n",
    "    (lowFilter, highFilter, median) = get_iqr_filter(df, col)\n",
    "    return df.withColumn(\n",
    "                    col,\n",
    "                    F.when(\n",
    "                        F.col(col) > highFilter,\n",
    "                        median\n",
    "                    ).otherwise( F.col(col) )\n",
    "        )\n",
    "\n",
    "def impute_item_attempt_duration(df):\n",
    "    return iqr_impute(df, 'item_attempt_duration_mins')\n",
    "\n",
    "def impute_student_duration(df):\n",
    "    return iqr_impute(df, 'student_duration_mins')\n",
    "\n",
    "def impute_timeliness_duration(df):\n",
    "    return iqr_impute(df, 'timeliness_duration_mins')\n",
    "\n",
    "def remove_unassigned_response_correctness(df):\n",
    "    return filterDf.filter(\n",
    "          ( F.col('response_correctness').isNull() == True )\n",
    "        | ( F.col('response_correctness') != '[unassigned]' )\n",
    "    )\n",
    "\n",
    "def remove_null_response_correctness(df):\n",
    "    return df.filter( F.col('response_correctness').isNull() == False )\n",
    "\n",
    "\n",
    "def swoe(df):\n",
    "    # Ref: https://www.researchgate.net/profile/Goutam_Chakraborty4/publication/259184404_Extension_Node_to_the_Rescue_of_the_Curse_of_Dimensionality_via_Weight_of_Evidence_WOE_Recoding/links/0c96052a3a083b6ccd000000.pdf\n",
    "\n",
    "    # Smoothing parameter.  large = aggressive, small = sensitive to data\n",
    "    c = 24\n",
    "    # Get portion of events\n",
    "    p1 = df.withColumn(\n",
    "        'target',\n",
    "        F.when( F.col('raw_score') > 0, 1).otherwise(0)\n",
    "    ).select('target').toPandas().target.mean()\n",
    "\n",
    "    # Get event and non event counts\n",
    "    cntDf =  df.groupBy('item_type_code_name').agg(\n",
    "        F.sum( F.when( F.col('raw_score')  > 0, 1 ).otherwise(0)).alias('1'),\n",
    "        F.sum( F.when( F.col('raw_score') == 0, 1 ).otherwise(0)).alias('0')\n",
    "    )\n",
    "    # Add swoe\n",
    "    # Formula:\n",
    "    #           # events + cp1\n",
    "    # ln( ----------------------- )\n",
    "    #     # nonevents + c(1 - p1)\n",
    "    cntDf = cntDf.withColumn(\n",
    "        'item_type_code_name_swoe',\n",
    "        F.log( (F.col('1') +  c * p1)  / (F.col('0') + c * (1 - p1)))\n",
    "    )\n",
    "    return df.join(cntDf.select('item_type_code_name', 'item_type_code_name_swoe'), 'item_type_code_name')\n",
    "\n",
    "def encode_categories_as_swoe(df):\n",
    "    return swoe(df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}