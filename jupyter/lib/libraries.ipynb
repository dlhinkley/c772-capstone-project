{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Temp for local development\n",
    "import ssl\n",
    "import os\n",
    "# os.environ['PYSPARK_PYTHON'] = '/Library/Frameworks/Python.framework/Versions/3.6/bin/python3.6'\n",
    "import tinydb as tinydb\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "types_global = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dython              0.6.1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# The Search for Categorical Correlation\n",
    "# https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
    "# https://github.com/shakedzy/dython\n",
    "# http://shakedzy.xyz/dython/\n",
    "if ! pip3 list | grep dython; then\n",
    "    pip3 install dython\n",
    "fi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinydb              4.2.0\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "if ! pip3 list | grep tinydb; then\n",
    "    pip3 install tinydb\n",
    "fi"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "\n",
    "from pyspark import SparkContext, SparkFiles, SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "# https://github.com/shakedzy/dython\n",
    "from dython.nominal import associations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SQLContext(sc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "dataDir = '/Users/duane.hinkley/PycharmProjects/c772-capstone-project/jupyter/.data/'\n",
    "\n",
    "if not os.path.exists(dataDir):\n",
    "    os.makedirs(dataDir)\n",
    "\n",
    "def init_df_raw():\n",
    "    rawDf = import_by_url('https://github.com/dlhinkley/c772-capstone-project/raw/master/data/assessment_items.csv')\n",
    "\n",
    "    # Save to reuse\n",
    "    save_df(rawDf, 'rawDf')\n",
    "\n",
    "    # Only keep \"fully scored\" items\n",
    "    # Filter to learner_attempt_status = 'fully scored'\n",
    "    filterDf = rawDf.filter(F.col('learner_attempt_status') == 'fully scored')\n",
    "\n",
    "    # Change Date Fields from String to Timestamp Type\n",
    "    types = get_var_types()\n",
    "    for f in types['intervalVars']:\n",
    "      filterDf = filterDf.withColumn(f, F.col(f).cast(T.TimestampType() ) )\n",
    "\n",
    "    # Set default date values to null (years 2999 and 1900)\n",
    "    # Set empty dates to null\n",
    "    for f in types['intervalVars']:\n",
    "      # Change to empty if date is more than 30 months in past or future\n",
    "      filterDf = filterDf.withColumn(f, F.when( F.abs(F.months_between(F.col(f), F.current_timestamp() )) > 30, None ).otherwise( F.col(f) ) )\n",
    "\n",
    "    # Save to reuse\n",
    "    save_df(filterDf, 'filterDf')\n",
    "\n",
    "    return filterDf\n",
    "\n",
    "\n",
    "def init_df_desc():\n",
    "    descDf = import_by_url('https://github.com/dlhinkley/c772-capstone-project/raw/master/data/descriptions.csv')\n",
    "    save_df(descDf, 'descDf')\n",
    "    return descDf\n",
    "\n",
    "\n",
    "def save_df(df, name):\n",
    "    df.repartition(1).write.mode('overwrite').parquet(dataDir + name + \".parquet\")\n",
    "\n",
    "def save_dict(data, name):\n",
    "    with open(dataDir + name + \".json\", \"w\") as f:\n",
    "      json.dump(data, f)\n",
    "\n",
    "def load_dict(name):\n",
    "    with open(dataDir + name + \".json\") as f:\n",
    "        out = json.load(f)\n",
    "\n",
    "    return out\n",
    "\n",
    "def load_df(name):\n",
    "    return spark.read.parquet(dataDir + name + \".parquet\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_non_string_vars():\n",
    "    global shared\n",
    "    return group.identifierVars + group.continuousVars + group.intervalVars + group.binaryVars\n",
    "\n",
    "def get_all_vars():\n",
    "        return group.nominalVars + get_non_string_vars();"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tinydb import TinyDB, Query\n",
    "# Create Todo list\n",
    "def init_todo():\n",
    "  global td, dataDir\n",
    "  td = TinyDB(dataDir + 'todo.json')\n",
    "\n",
    "def add_todo(desc):\n",
    "  global td\n",
    "  q = Query()\n",
    "  if not td.contains(q.todo == desc):\n",
    "      td.insert({'todo': desc, 'finished': False})\n",
    "\n",
    "def list_todo(finished = None):\n",
    "  global td\n",
    "\n",
    "  for item in td:\n",
    "    if (finished != None):\n",
    "        if (item['finished'] == finished):\n",
    "           print(item)\n",
    "    else:\n",
    "        print(item)\n",
    "\n",
    "def finish_todo(desc):\n",
    "  global td\n",
    "  q = Query()\n",
    "  td.update({'finished': True}, q.todo == desc)\n",
    "\n",
    "def delete_todo(desc):\n",
    "  global td\n",
    "  q = Query()\n",
    "  td.remove(q.todo == desc)\n",
    "\n",
    "\n",
    "init_todo()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark import SparkFiles\n",
    "from datetime import datetime\n",
    "\n",
    "def import_by_url(url):\n",
    "  # Given a url to a csv file, import and return a dataframe\n",
    "  #\n",
    "  sc.addFile(url)\n",
    "  filename = os.path.basename(url)\n",
    "  file = \"file://\" + SparkFiles.get(filename)\n",
    "  return spark.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(file)\n",
    "\n",
    "\n",
    "def filter_default(dfIn, f1, f2):\n",
    "  # Given a dataframe and two date field names, returns the dataframe removing records\n",
    "  # where the f1 or f2 columns equal a default date\n",
    "  defaultDates = [\"2999-01-01 00:00:00\", \"1900-01-01 00:00:00\"]\n",
    "  return dfIn.filter( ~F.col(f1).isin(defaultDates) & ~F.col(f2).isin(defaultDates) )\n",
    "\n",
    "\n",
    "def date_stats(dfIn, f1, f2):\n",
    "  # Given a dataframe and two date field names, returns a new dataframe with the difference between\n",
    "  # the dates in minutes, hours and minutes\n",
    "  dfOut = filter_default(dfIn, f1, f2)\n",
    "\n",
    "  dfOut = dfOut.withColumn(\"minues\", (F.col(f1).cast(\"long\") - F.col(f2).cast(\"long\"))/60.).select(f1, f2, \"minues\")\n",
    "\n",
    "  dfOut = dfOut.withColumn(\"hours\", (F.col(f1).cast(\"long\") - F.col(f2).cast(\"long\"))/3600.).select(f1, f2, \"hours\", \"minues\")\n",
    "\n",
    "  return dfOut.withColumn(\"days\", (F.col(f1).cast(\"long\") - F.col(f2).cast(\"long\"))/86400.).select(\"days\", \"hours\", \"minues\")\n",
    "\n",
    "\n",
    "def annotate_plot(ax):\n",
    "  # Add total labels to plot\n",
    "  for p in ax.patches:\n",
    "      ax.annotate(\n",
    "        round(p.get_height(), 2),\n",
    "        (p.get_x()+p.get_width()/2., p.get_height()),\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        color='white',\n",
    "        fontweight='bold',\n",
    "        xytext=(0, -10),\n",
    "        textcoords='offset points')\n",
    "\n",
    "\n",
    "def date_boxplot(df, title, ax = False):\n",
    "    # Given a dataframe of datestimes, create a boxplot of\n",
    "    # date distribution\n",
    "    types = get_var_types()\n",
    "    # Convert to timestamps\n",
    "    pdDf = df.select(* (F.unix_timestamp(c).alias(c) for c in types['intervalVars']) ).toPandas()\n",
    "\n",
    "    if ax:\n",
    "        pdDf.boxplot(rot=270, figsize=[10,10], ax = ax)\n",
    "    else:\n",
    "        ax = pdDf.boxplot(rot=270, figsize=[10,10])\n",
    "\n",
    "    # Min and Max date plus and minus one month\n",
    "    max = pd.to_datetime(pdDf.max().max(), unit='s') + pd.DateOffset(months=1)\n",
    "    min = pd.to_datetime(pdDf.min().min(), unit='s') - pd.DateOffset(months=1)\n",
    "\n",
    "    # Date labels by month\n",
    "    yLabels = pd.date_range(start=min.date(), end=max.date(), freq='MS')\n",
    "    # Convert ticks to unix timestamp (int)\n",
    "    ytick = [t.value // 10 ** 9 for t in yLabels]\n",
    "    # Year and month readable labels\n",
    "    newLabels = [ts.strftime('%Y-%m') for ts in yLabels]\n",
    "    ax.set_yticks(ytick)\n",
    "    ax.set_yticklabels(labels = newLabels)\n",
    "    ax.set_xticklabels(labels = types['intervalVarsLabels'])\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def type_cat(var):\n",
    "    # return types_global[var]\n",
    "    return var\n",
    "\n",
    "# Create a udf for pyspark\n",
    "type_cat_udf = F.udf(type_cat)\n",
    "\n",
    "\n",
    "def distinct_val(df, skipKnown = True):\n",
    "\n",
    "    # Given a dataframe, return the distinct values\n",
    "    knowCols = ['assignment_late_submission',\n",
    "                'learner_attempt_status',\n",
    "                'is_deleted']\n",
    "    if (skipKnown):\n",
    "        cols = blacklist(df.columns, knowCols)\n",
    "    else:\n",
    "        cols = df.columns\n",
    "\n",
    "    # Get count of values in each column\n",
    "    svDf = df.agg(*(F.countDistinct( F.when( F.col(c).isNull(), 'null').otherwise( F.col(c).cast('string') ) ).alias( c ) for c in cols))\n",
    "\n",
    "    # Save dataframe to list\n",
    "    sv = svDf.collect()[0]\n",
    "    # Get columns with a count of one\n",
    "    svCols = [c for c in cols if sv[c] == 1]\n",
    "    svCols.sort()\n",
    "    # Return one row of panda dataframe with count of 1\n",
    "    pdDf =  df.select( svCols ).limit(1).toPandas()\n",
    "    # Change the variable names to variables with labels\n",
    "    return col_to_label( pdDf ).transpose()\n",
    "\n",
    "\n",
    "def id_to_name(df, idVar, newVar, newIdList):\n",
    "  # Given a dataframe, id variable, new variable name and list of new ids\n",
    "  # add a new variable to the dataframe mapping the id to the array\n",
    "\n",
    "  # Save org ids to a list\n",
    "  oldIdList = [row[idVar] for row in df.select(idVar).distinct().orderBy(idVar).collect()]\n",
    "\n",
    "    # Create map\n",
    "  newIdMap = dict()\n",
    "  # Add letters to map\n",
    "  for i, val in enumerate(oldIdList):\n",
    "      newIdMap[val] = newIdList[i]\n",
    "\n",
    "  # Create mapping expression\n",
    "  mapping_expr = F.create_map([F.lit(x) for x in chain(*newIdMap.items())])\n",
    "\n",
    "  # Add org column with letter related to id\n",
    "  return df.withColumn(newVar, mapping_expr[df[idVar]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Return elements in whitelist\n",
    "def whitelist(l, whitelist):\n",
    "    if whitelist:\n",
    "        return l[ np.isin(whitelist)]\n",
    "    else:\n",
    "        return l\n",
    "\n",
    "# Return elements except those in blacklist\n",
    "def blacklist(l, blacklist):\n",
    "    if blacklist:\n",
    "        return [x for x in l if x not in blacklist]\n",
    "    else:\n",
    "        return l\n",
    "\n",
    "\n",
    "# Return a dictionary including arrays of variable names for each category\n",
    "# If dfColumns provied, return only values in dfColumns\n",
    "def get_var_cats(dfColumns = False):\n",
    "    cat = dict()\n",
    "    dfPd = load_df('descDf').toPandas()\n",
    "\n",
    "    cat['orgVars']               = whitelist( dfPd.loc[ dfPd['category'] == 'Organization' ].field.tolist(), dfColumns)\n",
    "    cat['sectionVars']           = whitelist( dfPd.loc[ dfPd['category'] == 'Section' ].field.tolist(), dfColumns)\n",
    "    cat['learnerVars']           = whitelist( dfPd.loc[ dfPd['category'] == 'Learner' ].field.tolist(), dfColumns)\n",
    "    cat['assessmentVars']        = whitelist( dfPd.loc[ dfPd['category'] == 'Assessment' ].field.tolist(), dfColumns)\n",
    "    cat['assignmentVars']        = whitelist( dfPd.loc[ dfPd['category'] == 'Assignment' ].field.tolist(), dfColumns)\n",
    "    cat['itemVars']              = whitelist( dfPd.loc[ dfPd['category'] == 'Item' ].field.tolist(), dfColumns)\n",
    "    cat['assignmentAttemptVars'] = whitelist( dfPd.loc[ dfPd['category'] == 'Assignment Attempt' ].field.tolist(), dfColumns)\n",
    "    cat['itemAttemptVars']       = whitelist( dfPd.loc[ dfPd['category'] == 'Item Attempt' ].field.tolist(), dfColumns)\n",
    "\n",
    "    return cat\n",
    "\n",
    "# Return a dictionary including arrays of variable names for each type\n",
    "# If dfColumns provied, return only values in dfColumns\n",
    "def get_var_types(dfColumns = False):\n",
    "\n",
    "    if (types_global == None):\n",
    "\n",
    "        type = dict()\n",
    "        descDf = load_df('descDf')\n",
    "\n",
    "        type['identifierVars']  = whitelist( variable_types_label(descDf, 'Categorical Identifier').variable.tolist(), dfColumns)\n",
    "        type['nominalVars']     = whitelist( variable_types_label(descDf, 'Categorical Nominal').variable.tolist(),  dfColumns)\n",
    "        type['continuousVars']  = whitelist( variable_types_label(descDf, 'Numeric Continuous').variable.tolist(),   dfColumns)\n",
    "        type['intervalVars']    = whitelist( variable_types_label(descDf, 'Categorical Interval').variable.tolist(), dfColumns)\n",
    "        type['binaryVars']      = whitelist( variable_types_label(descDf, 'Categorical Binary').variable.tolist(),   dfColumns)\n",
    "\n",
    "        type['identifierVarsLabels']  = whitelist( variable_types_label(descDf, 'Categorical Identifier').variable_label.tolist(), dfColumns)\n",
    "        type['nominalVarsLabels']     = whitelist( variable_types_label(descDf, 'Categorical Nominal').variable_label.tolist(),  dfColumns)\n",
    "        type['continuousVarsLabels']  = whitelist( variable_types_label(descDf, 'Numeric Continuous').variable_label.tolist(),   dfColumns)\n",
    "        type['intervalVarsLabels']    = whitelist( variable_types_label(descDf, 'Categorical Interval').variable_label.tolist(), dfColumns)\n",
    "        type['binaryVarsLabels']      = whitelist( variable_types_label(descDf, 'Categorical Binary').variable_label.tolist(),   dfColumns)\n",
    "\n",
    "        for cat in ['identifierVars', 'nominalVars', 'continuousVars', 'intervalVars', 'binaryVars']:\n",
    "            for p in range( len( type[cat] ) ):\n",
    "                type[ type[cat][p] ] = type[ cat + 'Labels'][p]\n",
    "\n",
    "        return type\n",
    "    else:\n",
    "        return types_global\n",
    "\n",
    "\n",
    "# Given a dataframe of datetime fields, return a matrix of the mean difference\n",
    "#\n",
    "def date_diff_map(df, title, scale = 'D', ax = None):\n",
    "\n",
    "    intVars = df.columns\n",
    "    intSize = len(intVars)\n",
    "    am      = pd.DataFrame(np.zeros(shape=(intSize , intSize)), columns = intVars, index = intVars)\n",
    "\n",
    "\n",
    "    for v1 in intVars:\n",
    "        for v2 in intVars:\n",
    "            if v1 != v2:\n",
    "\n",
    "                mean = ( ( df[v1] - df[v2] ) / np.timedelta64(1,scale) ).mean()\n",
    "                am.at[v1,v2] = mean\n",
    "\n",
    "    ax = sn.heatmap(am, annot=True, fmt=\".0f\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "\n",
    "def get_random_sample(df):\n",
    "    return df.sample(False, .10, 8764664)\n",
    "\n",
    "def mean_hours_assignment_interval(df, ax=None):\n",
    "    assignIntVars = [\n",
    "        'student_start_datetime',\n",
    "        'was_in_progress_datetime',\n",
    "        'scored_datetime',\n",
    "        'was_submitted_datetime_actual',\n",
    "        'student_stop_datetime',\n",
    "        'was_fully_scored_datetime',\n",
    "    ]\n",
    "    date_diff_map(df.select(assignIntVars).toPandas(), \"Mean Hours Between Assignment Interval Vars\", 'h', ax)\n",
    "\n",
    "\n",
    "def dual_mean_hours_assignment(df1, df2, title1='', title2='', main=''):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, sharex=True)\n",
    "\n",
    "    mean_hours_assignment_interval(df1, ax1)\n",
    "    ax1.set_title(title1)\n",
    "\n",
    "    mean_hours_assignment_interval(df2, ax2)\n",
    "    ax2.set_title(title2)\n",
    "\n",
    "    plt.suptitle(main)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def impute_749_to_750_null_dates(filterDf):\n",
    "\n",
    "    # Get sample to extract means\n",
    "    pdDf = get_random_sample(filterDf).select('student_start_datetime', 'student_stop_datetime', 'was_fully_scored_datetime', 'scored_datetime').toPandas()\n",
    "\n",
    "    # Calculate mean difference in seconds\n",
    "    mStStartDate = ( (pdDf['scored_datetime'] - pdDf['student_start_datetime'])     / np.timedelta64(1, 's') ).mean()\n",
    "    mStStopDate  = ( (pdDf['scored_datetime'] - pdDf['student_stop_datetime'])      / np.timedelta64(1, 's') ).mean()\n",
    "    mWFullScored = ( (pdDf['scored_datetime'] - pdDf['was_fully_scored_datetime'])  / np.timedelta64(1, 's') ).mean()\n",
    "\n",
    "\n",
    "    return filterDf.withColumn(\n",
    "                \"student_start_datetime\",\n",
    "                F.when(\n",
    "                    F.col(\"student_start_datetime\").isNull(),\n",
    "                    (F.unix_timestamp(\"scored_datetime\") - mStStartDate).cast('timestamp')\n",
    "                ).otherwise( F.col(\"student_start_datetime\") )\n",
    "            ).withColumn(\n",
    "                \"student_stop_datetime\",\n",
    "                F.when(\n",
    "                    F.col(\"student_stop_datetime\").isNull(),\n",
    "                    (F.unix_timestamp(\"scored_datetime\") - mStStopDate).cast('timestamp')\n",
    "                ).otherwise( F.col(\"student_stop_datetime\") )\n",
    "            ).withColumn(\n",
    "                \"was_fully_scored_datetime\",\n",
    "                F.when(\n",
    "                    F.col(\"was_fully_scored_datetime\").isNull(),\n",
    "                    (F.unix_timestamp(\"scored_datetime\") - mWFullScored).cast('timestamp')\n",
    "                ).otherwise( F.col(\"was_fully_scored_datetime\") )\n",
    "    )\n",
    "\n",
    "\n",
    "def impute_3422_null_dates(filterDf):\n",
    "\n",
    "    # Get sample to extract means\n",
    "    pdDf = get_random_sample(filterDf).select('student_start_datetime', 'student_stop_datetime', 'was_fully_scored_datetime', 'scored_datetime').toPandas()\n",
    "\n",
    "    # Calculate mean difference in seconds\n",
    "    mScoredDate = ( (pdDf['scored_datetime'] - pdDf['student_stop_datetime'])     / np.timedelta64(1, 's') ).mean()\n",
    "\n",
    "\n",
    "    return filterDf.withColumn(\n",
    "                \"scored_datetime\",\n",
    "                F.when(\n",
    "                    F.col(\"scored_datetime\").isNull(),\n",
    "                    (F.unix_timestamp(\"student_stop_datetime\") - mScoredDate).cast('timestamp')\n",
    "                ).otherwise( F.col(\"scored_datetime\") )\n",
    "    )\n",
    "\n",
    "\n",
    "def impute_9965_null_dates(df):\n",
    "\n",
    "    # Get sample to extract means\n",
    "    pdDf = get_random_sample(df).select('student_start_datetime', 'student_stop_datetime', 'was_in_progress_datetime', 'scored_datetime').toPandas()\n",
    "\n",
    "    # Calculate mean difference in seconds\n",
    "    meanDiff = ( (pdDf['was_in_progress_datetime'] - pdDf['student_start_datetime'])     / np.timedelta64(1, 's') ).mean()\n",
    "\n",
    "\n",
    "    return df.withColumn(\n",
    "                \"scored_datetime\",\n",
    "                F.when(\n",
    "                    F.col(\"was_in_progress_datetime\").isNull(),\n",
    "                    (F.unix_timestamp(\"student_start_datetime\") - meanDiff).cast('timestamp')\n",
    "                ).otherwise( F.col(\"was_in_progress_datetime\") )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Given a dataframe and variable name, return the value names and counts for that variable\n",
    "def count_values(df, f):\n",
    "   return df.groupBy(f).count().orderBy('count', ascending=False)\n",
    "\n",
    "\n",
    "def response_correctness_bar_plot(df, varName, ax=None):\n",
    "    pdDf = df.groupBy(varName).count().orderBy('count', ascending=False).toPandas()\n",
    "    ax = pdDf.plot(kind='bar', ax=ax)\n",
    "    labels = pdDf[varName].fillna('null').tolist()\n",
    "    ax.set_xticklabels(labels=labels)\n",
    "    ax.set_title('Values ' + varName)\n",
    "    annotate_plot(ax)\n",
    "\n",
    "\n",
    "# Create a binary catagorical variable for final_score_unweighted\n",
    "def add_zero_final_score_var(df):\n",
    "    return df.withColumn(\n",
    "                \"zero_score\",\n",
    "                  F.when( F.col('final_score_unweighted') == 0, 'Yes').otherwise(\"No\")\n",
    "                )\n",
    "\n",
    "def add_zero_raw_score_var(df):\n",
    "    return df.withColumn(\n",
    "                \"zero_score\",\n",
    "                  F.when( F.col('raw_score').isNull(), 'Null')\n",
    "                      .otherwise( F.when( F.col('raw_score') == 0, 'Yes').otherwise(\"No\")   )\n",
    "                )\n",
    "\n",
    "\n",
    "# Create a bar chart counting countvar for each groupvar\n",
    "def num_group_bar_chart(df, groupByVar, countVar, countAlias, title, ax = None):\n",
    "  sByO = df.groupBy(groupByVar).agg(F.countDistinct(countVar).alias(countAlias)).orderBy(groupByVar)\n",
    "  pdDf = sByO.toPandas()\n",
    "\n",
    "  # Add mean\n",
    "  mean = sByO.agg(F.round(F.avg(F.col(countAlias))).alias('mean')).collect()[0][0]\n",
    "  # Append row with mean\n",
    "  pdDf = pdDf.append({groupByVar: 'mean', countAlias: mean}, ignore_index=True)\n",
    "\n",
    "  axa = pdDf.plot(groupByVar,countAlias, kind='bar', ax=ax, title=title)\n",
    "  annotate_plot(axa)\n",
    "  if (ax == None):\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def num_sections_by_org_bar_chart(df, ax = None, title='Num Sections by Organization'):\n",
    "    num_group_bar_chart(df, 'org_id', 'section_id', 'sections', title, ax)\n",
    "\n",
    "\n",
    "def num_learners_by_org_bar_chart(df):\n",
    "    num_group_bar_chart(df, 'org_id', 'learner_id', 'learners', 'Num Learners by Organization')\n",
    "\n",
    "\n",
    "def mean_group_bar_chart(df, group1, group2, countVar, countAlias, title='', ax=None):\n",
    "  lByS = df.groupBy(group1, group2).agg(F.countDistinct(countVar).alias(countAlias))\n",
    "  # Av\n",
    "  lBySMean = lByS.groupBy(group1).agg( F.avg(countAlias).alias(countAlias) ).orderBy(group1)\n",
    "  pdDf = lBySMean.toPandas()\n",
    "\n",
    "  # Add mean\n",
    "  mean = lBySMean.agg(F.round(F.avg(F.col(countAlias))).alias('mean')).collect()[0][0]\n",
    "  # Append row with mean\n",
    "  pdDf = pdDf.append({group1: 'mean', countAlias: mean}, ignore_index=True)\n",
    "\n",
    "  axo = pdDf.plot.bar(group1,countAlias, ax=ax, title=title)\n",
    "  annotate_plot(axo)\n",
    "\n",
    "  if (ax == None):\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def mean_sec_learners_by_org_bar_chart(df):\n",
    "    mean_group_bar_chart(df, 'org_id','section_id', 'learner_id', 'learners', 'Mean Section Learners by Organization')\n",
    "\n",
    "\n",
    "def mean_sec_assess_by_org_bar_chart(df, ax=None, title='Mean Section Assessments by Organization'):\n",
    "    mean_group_bar_chart(df, 'org_id','section_id', 'assessment_id', 'assessments', ax=ax, title=title)\n",
    "\n",
    "\n",
    "def mean_assess_by_org_bar_chart(df, ax=None, title = 'Mean Learners Assessments by Organization'):\n",
    "    mean_group_bar_chart(df, 'org_id','learner_id', 'assessment_id', 'assessments',ax=ax, title=title,)\n",
    "\n",
    "# Mean Scores by Organization\n",
    "def mean_scores_by_orgs_bar_chart(df):\n",
    "  sByO = df.groupBy('org_id').agg( F.avg('final_score_unweighted').alias('scores') ).orderBy('org_id')\n",
    "\n",
    "  pdDf = sByO.toPandas()\n",
    "\n",
    "  # Add mean\n",
    "  meanAssess = sByO.agg(F.round(F.avg(F.col('scores'))).alias('mean')).collect()[0][0]\n",
    "  # Append row with mean\n",
    "  pdDf = pdDf.append({'org_id': 'mean', 'scores': meanAssess}, ignore_index=True)\n",
    "\n",
    "  ax = pdDf.plot.bar('org_id','scores', title='Mean Scores by Organization')\n",
    "  annotate_plot(ax)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def crosstab_percent(table):\n",
    "    return table.apply(lambda r: round(r/r.sum() * 100), axis=1)\n",
    "\n",
    "\n",
    "# Given two dataframes of datetime fields, with optional titles, return two side by side boxplots\n",
    "#\n",
    "def dual_date_boxplot(df1, df2, title1='', title2='', main=''):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, sharex=True)\n",
    "\n",
    "    date_boxplot( df1, title1, ax1)\n",
    "    date_boxplot(df2, title2, ax2)\n",
    "\n",
    "    plt.suptitle(main)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Given two pandas dataframes with one variable each, return two histograms\n",
    "def dual_hist(pdDf1, pdDf2, title1='', title2='', main=''):\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\n",
    "\n",
    "    pdDf1.hist(ax=ax1)\n",
    "    ax1.set_title(title1)\n",
    "\n",
    "    pdDf2.hist(ax=ax2)\n",
    "    ax2.set_title(title2)\n",
    "\n",
    "    plt.suptitle(main)\n",
    "    plt.show()\n",
    "\n",
    "# Given two dataframes, create two heatmaps of all variables in dataframe\n",
    "def dual_assoc_heatmap(df1, df2, title1 = '', title2 = '', main = '', figsize=(10,5)):\n",
    "    fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True, sharex=True, figsize=figsize)\n",
    "\n",
    "    associations( df1.toPandas(), nan_replace_value='null', ax=ax1, plot=False )\n",
    "    ax1.set_title(title1)\n",
    "\n",
    "    associations( df2.toPandas(), nan_replace_value='null', ax=ax2, plot=False )\n",
    "    ax2.set_title(title2)\n",
    "\n",
    "    plt.suptitle(main)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Impute the 4446 null dates with the mean difference of was_submitted_datetime_actual and student_stop_datetime\n",
    "def impute_4446_null_dates(df):\n",
    "\n",
    "    # Get sample to extract means\n",
    "    pdDf = df.select( 'scored_datetime', 'was_submitted_datetime_actual').toPandas()\n",
    "\n",
    "    # Calculate mean difference in seconds\n",
    "    meanDiff = ( (pdDf['scored_datetime'] - pdDf['was_submitted_datetime_actual'])     / np.timedelta64(1, 's') ).mean()\n",
    "\n",
    "\n",
    "    return df.withColumn(\n",
    "                \"was_submitted_datetime_actual\",\n",
    "                F.when(\n",
    "                    (F.col('was_submitted_datetime_actual').isNull()) & (F.col('final_score_unweighted') > 0),\n",
    "                    (F.unix_timestamp(\"scored_datetime\") - meanDiff).cast('timestamp')\n",
    "                ).otherwise( F.col(\"was_submitted_datetime_actual\") )\n",
    "    )\n",
    "\n",
    "\n",
    "def impute_number_of_learners(cleanDf):\n",
    "\n",
    "    # Calculate number of learners on Filtered\n",
    "    dfCount = cleanDf.groupBy('assessment_instance_id', 'number_of_learners').agg(\n",
    "        F.countDistinct('learner_id').alias('number_of_learners_calc')\n",
    "    ).select('assessment_instance_id', 'number_of_learners_calc')\n",
    "\n",
    "    # Update with calculated value\n",
    "    cleanDf = cleanDf.join(dfCount, on=['assessment_instance_id'], how='left')\n",
    "\n",
    "    # Drop incorrect number of learners\n",
    "    cleanDf = cleanDf.drop('number_of_learners')\n",
    "\n",
    "    # Rename calculated to original\n",
    "    return cleanDf.withColumnRenamed(\"number_of_learners_calc\",\"number_of_learners\")\n",
    "\n",
    "\n",
    "# Reduce the number of levels in item_type_code_name\n",
    "def reduce_type_code_levels(cleanDf):\n",
    "    # Combine fillInTheBlank and FillinBlankResponse\n",
    "    cleanDf = cleanDf.withColumn(\"item_type_code_name\", F.when( F.col(\"item_type_code_name\") == \"FillinBlankResponse\", \"fillInTheBlank\" ).otherwise(F.col(\"item_type_code_name\")) )\n",
    "\n",
    "    # Combine multipleChoice and MultipleChoiceResponse\n",
    "    cleanDf = cleanDf.withColumn(\"item_type_code_name\", F.when( F.col(\"item_type_code_name\") == \"MultipleChoiceResponse\", \"multipleChoice\" ).otherwise(F.col(\"item_type_code_name\")) )\n",
    "\n",
    "    # Total count\n",
    "    tot = cleanDf.filter(F.col(\"item_type_code_name\").isNull() == False).count()\n",
    "\n",
    "    freqTable = cleanDf.groupBy(\"item_type_code_name\") \\\n",
    "                   .count() \\\n",
    "                   .withColumnRenamed('count', 'cnt_per_group') \\\n",
    "                   .withColumn('perc_of_count_total', ( F.col('cnt_per_group') / tot) * 100 ) \\\n",
    "                   .orderBy(\"cnt_per_group\", ascending=False)\n",
    "\n",
    "    # freqTable.show(50, False)\n",
    "\n",
    "    # We only want five levels, so convert everything below 6% to other\n",
    "\n",
    "\n",
    "    otherRows    = freqTable.filter(\"perc_of_count_total < 6\")\n",
    "    otherLevels  = [row['item_type_code_name'] for row in otherRows.select(\"item_type_code_name\").collect()]\n",
    "\n",
    "    return cleanDf.withColumn(\"item_type_code_name\", F.when( F.col(\"item_type_code_name\").isin(otherLevels) | F.col(\"item_type_code_name\").isNull() , \"Other\" ).otherwise(F.col(\"item_type_code_name\")) )\n",
    "\n",
    "\n",
    "# Return dataframe of min, max, mean grouped by column\n",
    "def group_by_describe(df, groupBy, statsCol):\n",
    "    return df.groupBy(groupBy).agg(\n",
    "          F.round( F.count(statsCol) ).alias('count'),\n",
    "          F.round( F.min(statsCol) ).alias('min'),\n",
    "          F.round( F.avg(statsCol) ).alias('mean'),\n",
    "          F.round( F.max(statsCol) ).alias('max')\n",
    "    )\n",
    "\n",
    "\n",
    "# Given a dataframe and list of cols, displays the min, max, null, and unique value counts\n",
    "def date_min_max_null_unique(df, cols):\n",
    "    for f in cols:\n",
    "      print (f)\n",
    "      df.agg(\n",
    "        F.countDistinct(f).alias('unique'),\n",
    "        F.count(F.when(F.col(f).isNull(), f)).alias('null'),\n",
    "        F.min(f).alias('min'),\n",
    "        F.max(f).alias('max')\n",
    "     ).show(1, False)\n",
    "\n",
    "# Given a dataframe of dates and columns, return a pandas datafram of statistics\n",
    "def date_statisticts(df, cols):\n",
    "    cols.sort()\n",
    "    distinct = df.agg(\n",
    "        *(F.countDistinct(F.col(c)).alias(c) for c in cols)\n",
    "    ).collect()[0]\n",
    "    null = df.agg(\n",
    "        *(F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in cols)\n",
    "    ).collect()[0]\n",
    "    min = df.agg(\n",
    "        *(F.min(F.col(c).cast(T.DateType())).alias(c) for c in cols)\n",
    "    ).collect()[0]\n",
    "    max = df.agg(\n",
    "        *(F.max(F.col(c).cast(T.DateType())).alias(c) for c in cols)\n",
    "    ).collect()[0]\n",
    "    plotdata = pd.DataFrame({\n",
    "        \"distinct\": distinct,\n",
    "        \"null\":null,\n",
    "        \"min\":min,\n",
    "        \"max\":max,\n",
    "    },\n",
    "    index=cols)\n",
    "\n",
    "    return plotdata\n",
    "\n",
    "\n",
    "def null_zero_counts(df, cols):\n",
    "    cols.sort()\n",
    "    null = df.agg(\n",
    "        *(F.count(F.when(F.col(c).isNull(), c)).alias('null') for c in cols)\n",
    "    ).collect()[0]\n",
    "    zero = df.agg(\n",
    "        *(F.count(F.when(F.col(c) == 0, c)).alias(\"zero\") for c in cols)\n",
    "    ).collect()[0]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"null\": null,\n",
    "        \"zero\": zero,\n",
    "    }, index=cols)\n",
    "\n",
    "\n",
    "def unique_nulls(df, cols):\n",
    "    cols.sort()\n",
    "    null = df.agg(\n",
    "        *(F.count(F.when(F.col(c).isNull(), c)).alias('null') for c in cols)\n",
    "    ).collect()[0]\n",
    "    unique = df.agg(\n",
    "        *(F.countDistinct(c).alias('unique') for c in cols)\n",
    "    ).collect()[0]\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"null\": null,\n",
    "        \"unique\": unique,\n",
    "    }, index=cols)\n",
    "\n",
    "# Given a full name of multiple words, return the initials in lower case\n",
    "def initials(fullname):\n",
    "  xs = (fullname)\n",
    "  name_list = xs.split()\n",
    "\n",
    "  initials = \"\"\n",
    "\n",
    "  for name in name_list:  # go through each name\n",
    "    initials += name[0].lower()  # append the initial\n",
    "\n",
    "  return initials\n",
    "\n",
    "# Given a full name of multiple words,\n",
    "# return the initials in lower case wrapped in parenthesis\n",
    "def wrap_initials(fullname):\n",
    "    return '(' + initials(fullname) + ')'\n",
    "\n",
    "# Create a udf for pyspark\n",
    "initials_udf = F.udf(wrap_initials)\n",
    "\n",
    "#Given a dataframe and variable type, return the names of variables\n",
    "def variable_types(df, type):\n",
    "    return df.filter(F.col('type') == type ).select(\n",
    "            F.concat_ws(' ', F.col('category'),initials_udf( F.col('category') ) ).alias('category'),\n",
    "            F.col('field').alias('variable')\n",
    "        ).orderBy('category', 'variable').toPandas()\n",
    "\n",
    "\n",
    "#Given a dataframe and variable type, return the names of\n",
    "# variables with cat label as pandas dataframe\n",
    "def variable_types_label(df, type = None):\n",
    "    return df.filter(F.col('type') == type ).select(\n",
    "            F.concat_ws(' ', F.col('field'),initials_udf( F.col('category') ) ).alias('variable_label'),\n",
    "            F.col('field').alias('variable')\n",
    "        ).orderBy('category', 'variable').toPandas()\n",
    "\n",
    "\n",
    "# Given a pandas dataframe remap the column name to columns with labels\n",
    "def col_to_label(pdDf):\n",
    "    typeDict = get_var_types()\n",
    "    pdDf.columns = pdDf.columns.to_series().map(typeDict)\n",
    "    return pdDf\n",
    "\n",
    "\n",
    "\n",
    "get_var_types()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}